{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ai06_김태호_project4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPMizwmieIwk"
      },
      "source": [
        "# 심리상담 데이터를 통해서 심리상담을 해주는 모델 만들기\n",
        "##(TASK : 소설 창작과 같은 seqGAN에서 착안하여 주어진 몇개의 단어를 가지고 상담자의 심리를 파악하여 심리상담 글을 작성해주기)\n",
        "\n",
        "목차\n",
        "1. 사용할 데이터 가져오기(웹크롤링,네이버지식인 서울시청년사이버상담내역)\n",
        "2. 데이터 전처리 및 데이터 벡터화(Fasttext를 사용)\n",
        "3. seqGAN(생성자,감별자)를 사용하여 상담글 생성하기(미완)\n",
        "\n",
        "한계점 및 아쉬운점\n",
        "1. 적절한 데이터를 찾는 데 너무 많은 시간을 소비해서 제대로 모델을 만져보지 못했다.\n",
        "2. seqGAN에 대한 예시나 선행 과정이 한국어로는 거의 전무한 상태인 것 같다.\n",
        "3. 한국어를 자소분리한 형태로 FASTTEXT에 활용해보려고 했으나 현재 내가 찾은 seqGAN모델이 Word2vec을 사용하여 vocab을 만들어서 활용하는 모델이였고 찾아본 다른 프로젝트에서도 비슷한 구조로 진행을 하는 것 같다. 더 좋은 성능을 발휘하지 않을까해서 시도해본 것인데 내 실력으로 전체적인 GAN 구조를 바꾸면서 진행을 해야하기에는 무리였던 것 같다.\n",
        "4. 이 모델을 완성하여 어느정도 비슷한 상담내용을 만들 수 있더라도 과연 정신적으로 불안하며 많은 변수가 존재하는 인간의 심리를 인공지능이 맞출 수 있을까라는 한계점이 보였다. 또한, 불안정한 심리를 거스르는 것이야 말로 독이 되는 행위이기때문에 암환자 양성검사와 같이 재현율(Recall)을 높히는 방향으로의 설계가 필요하다고 생각되었습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_oVQecgR3ifS"
      },
      "source": [
        "###1.데이터 생성하기\n",
        "- 사용할 데이터를 수집하기 위해 웹크롤링을 통해 네이버지식인의 서울시청년사이버상담의 2021년 답변을 모두 스크래핑하여 훈련데이터를 구성하기로 했습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0fJTsqa3btd"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import f1_score,accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "#from category_encoders import OrdinalEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "import sqlite3\n",
        "import joblib\n",
        "import pickle\n",
        "import re"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHYKzvjXdmYF"
      },
      "source": [
        "#데이터 불러오기\n",
        "conn= sqlite3.connect(\"/content/drive/MyDrive/코드스테이츠 과제/project4/url_data.db\")\n",
        "cur = conn.cursor()\n",
        "query = cur.execute(\"SELECT * FROM dataset\")\n",
        "cols = [column[0] for column in query.description]\n",
        "df = pd.DataFrame.from_records(data=query.fetchall(),columns=cols)\n",
        "#데이터베이스 닫기\n",
        "conn.close()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "8jn4vuiQ6i0c",
        "outputId": "44ceff7b-c1f1-4e74-f7db-917a16b7dcc7"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>counseling</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>안녕하세요, 서울시청소년상담복지센터 사이버상담원입니다.​​song*...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>너무 성적에 연연하지 마시고 틀릴때마다 약간 완벽하게 하지 못해서 ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>숙제를 열심히 한것 아닐까요? 원래 열심히 할 수록 잘못되면 짜증나...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>학교 상담하는곳 있음.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>상담 자체로도 큰 효과를 볼 수 있는 경우도 많기에 학생이시라면 학...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          counseling\n",
              "0           안녕하세요, 서울시청소년상담복지센터 사이버상담원입니다.​​song*...\n",
              "1           너무 성적에 연연하지 마시고 틀릴때마다 약간 완벽하게 하지 못해서 ...\n",
              "2           숙제를 열심히 한것 아닐까요? 원래 열심히 할 수록 잘못되면 짜증나...\n",
              "3                                  학교 상담하는곳 있음.     \n",
              "4           상담 자체로도 큰 효과를 볼 수 있는 경우도 많기에 학생이시라면 학..."
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atGoYid197fc",
        "outputId": "c9337aae-0ff7-42dd-efdc-b8affb64fe72"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1089 entries, 0 to 1088\n",
            "Data columns (total 1 columns):\n",
            " #   Column      Non-Null Count  Dtype \n",
            "---  ------      --------------  ----- \n",
            " 0   counseling  1089 non-null   object\n",
            "dtypes: object(1)\n",
            "memory usage: 8.6+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktSx1yu-8V2o"
      },
      "source": [
        "#정규포현식 사용하여 공백 및 필요없는 내용 삭제하기 (/n,\\u200b)\n",
        "\n",
        "def preprocess_sentence_kr(w):\n",
        "  w = w.strip()\n",
        "  w = re.sub(r\"[^0-9가-힣?.!,¿]+\", \" \", w) # \\n도 공백으로 대체해줌\n",
        "  w = w.strip()\n",
        "  return w"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRxERmRc8WiP"
      },
      "source": [
        "df['counseling'] = [preprocess_sentence_kr(l) for l in df['counseling']]"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "McYVSlZ66lyd"
      },
      "source": [
        "#적용이 안됐음..\n",
        "#df['counseling']=df['counseling'].replace('\\u200b','')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "CRh1Znd08B1j",
        "outputId": "f50f226c-b096-4de2-8bba-6cf0cc27360c"
      },
      "source": [
        "df['counseling'][5]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'안녕하세요, 서울시청소년상담복지센터 사이버상담원입니다. 님이 올려주신 글을 잘 읽어보았어요. 자세한 내용은 모르겠지만, 님께 상처로 남은 일이 있을 것 같고, 마음이 아프고 괴롭겠어요. 그래서 심리상담을 받아보고 싶은데, 돈이 들지 않으면서 부모님께 알려지지 않는 곳은 없을 것 같아서 마음에 걸렸군요. 정보를 알려주고 싶어서 답변을 남겨요. 만 9세 24세 청소년은 시군구 청소년상담복지센터에서 무료 또는 저렴한 비용으로 상담을 받을 수 있어요. 상담을 시작할 때는 부모님 동의가 필요할 수 있지만, 자신이나 타인을 해칠 우려가 있는 경우가 아니면 상담 내용은 비밀이 보장돼요. 님 집 근처 센터에 전화로 문의하여 상담을 신청할 수 있고, 연락처는 아래 사이트 내용을 참고해주세요. 청소년상담복지센터 지역센터 안내 . . . . ? 1 상담을 받으면서 님 마음이 점차 괜찮아지고, 상담선생님과 함께 어려움을 풀어갈 수 있다면 좋겠어요. 아직 센터로 가기 어렵다면 저희 서울시청소년상담복지센터 홈페이지에 오셔서 게시판 상담과 채팅 상담 월, 목 12시 7시 가능 5시 6시 제외 을 먼저 하실 수도 있으니 참고해주세요. 제 답변이 조금이나마 도움이 되길 바랄게요. 서울시청소년상담복지센터 사이버상담원 드림'"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "id": "cJbscfA8B_3X",
        "outputId": "b34910fa-d112-4d1a-c310-66f2e54972be"
      },
      "source": [
        "df['counseling'][0]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'안녕하세요, 서울시청소년상담복지센터 사이버상담원입니다. 님이 올려주신 글을 잘 읽어보았어요. 내가 원하는대로 되지 않아서 짜증과 화가 나고, 괴로울 때가 있지요. 님은 요즘 주로 수학 문제를 풀고 나서 틀린 문제가 있으면 마음에 많이 걸리고, 괴롭군요. 그렇지만 완벽한 사람은 없기 때문에 누구나 실수를 하게 마련이고, 수학을 배워가는 과정에 있으므로 틀리는 문제가 있을 수도 있지요. 님은 공부를 완벽하게, 잘 하고 싶은 마음이 많으신 것 같아요. 잘 하고 싶은 마음이 있으면 학업에 도움이 되기도 하지만, 그러한 마음이 너무 커지면 스스로에게 기대가 너무 높아져서 부담스럽고, 괴로워지지요. 어쩌면 님 마음에 잘 해야 한다 는 기준이 높아서 몇 문제만 틀려도 화와 짜증이 나고, 눈물이 날 수도 있겠어요. 먼저 수학을 잘 하고 싶은데 잘 되지 않아서 화가 났지 하면서 자신의 마음을 이해해주면 좋겠어요. 한편, 틀린 문제를 살펴보면서 다음에는 어떻게 문제를 풀어볼지 생각해보고, 점차 실력을 쌓아가게 되지요. 틀린 문제 갯수보다는 배워가는 과정임을 떠올려보고, 실력을 더 쌓아보는 계기로 삼아보면 어떨까 싶어요. 문제를 틀릴 수도 있지, 괜찮아, 배워가는 중이야 하면서 스스로를 다독여주고, 좀 더 기회를 주는 연습을 해보면 좋겠어요. 제 답변이 조금이나마 위로가 되고, 도움이 되길 바랄게요. 만 9세 24세 청소년은 저희 서울시청소년상담복지센터를 이용하실 수 있으니 더 도움받고 싶은 주제가 있을 때 언제든 찾아주세요. 서울시청소년상담복지센터 사이버상담원 드림'"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHmVztnt_BpQ"
      },
      "source": [
        "#konlpy 설치\n",
        "%%bash\n",
        "apt-get update\n",
        "apt-get install g++ openjdk-8-jdk python-dev python3-dev\n",
        "pip3 install JPype1\n",
        "pip3 install konlpy\n",
        "%env JAVA_HOME \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "%%bash\n",
        "bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)\n",
        "pip3 install /tmp/mecab-python-0.996"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Du3VY_cs8oRr",
        "outputId": "5791303a-ed2f-4e77-a257-127726b1a140"
      },
      "source": [
        "# 토큰화 시키기 시험용\n",
        "from konlpy.tag import Kkma\n",
        "kkma=Kkma()\n",
        "print(kkma.morphs(df['counseling'][0]))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['안녕', '하세', '요', ',', '서울시', '청소년', '상담', '복지', '센터', '사이버', '상담원', '이', 'ㅂ니다', '.', '니', 'ㅁ', '이', '올리', '어', '주신', '글', '을', '잘', '읽', '어', '보', '았', '어요', '.', '내', '가', '원하', '는', '대로', '되', '지', '않', '아서', '짜증', '과', '화가', '나', '고', ',', '괴롭', 'ㄹ', '때', '가', '있', '지요', '.', '니', 'ㅁ', '은', '요즘', '주로', '수학', '문제', '를', '풀', '고', '나', '서', '틀리', 'ㄴ', '문제', '가', '있', '으면', '마음', '에', '많이', '걸리', '고', ',', '괴롭', '군요', '.', '그렇', '지만', '완벽', '하', 'ㄴ', '사람', '은', '없', '기', '때문', '에', '누구', '나', '실수', '를', '하', '게', '마련', '이', '고', ',', '수학', '을', '배워가', '는', '과정', '에', '있', '으므로', '틀리', '는', '문제', '가', '있', '을', '수', '도', '있', '지요', '.', '니', 'ㅁ', '은', '공부', '를', '완벽', '하', '게', ',', '잘', '하', '고', '싶', '은', '마음', '이', '많', '으시', 'ㄴ', '것', '같', '아요', '.', '잘', '하', '고', '싶', '은', '마음', '이', '있', '으면', '학업', '에', '도움', '이', '되', '기', '도', '하', '지만', ',', '그러하', 'ㄴ', '마음', '이', '너무', '커지', '면', '스스로', '에게', '기대', '가', '너무', '높', '아', '지', '어서', '부담', '스럽', '고', ',', '괴', '로', '워', '지', '지요', '.', '어쩌', '면', '님', '마음', '에', '잘', '하', '어야', '하', 'ㄴ다', '늘', 'ㄴ', '기준', '이', '높', '아서', '몇', '문제', '만', '틀리', '어도', '화', '와', '짜증', '이', '나', '고', ',', '눈물', '이', '나', 'ㄹ', '수', '도', '있', '겠', '어요', '.', '먼저', '수학', '을', '잘', '하', '고', '싶', '은데', '잘', '되', '지', '않', '아서', '화가', '나', '었', '지', '하', '면서', '자신', '의', '마음', '을', '이해', '하', '어', '주', '면', '좋', '겠', '어요', '.', '한편', ',', '틀리', 'ㄴ', '문제', '를', '살펴보', '면서', '다음', '에', '는', '어떻', '게', '문제', '를', '풀', '어', '보', 'ㄹ지', '생각', '하', '어', '보고', ',', '점차', '실력', '을', '쌓', '아', '가게', '되', '지요', '.', '틀리', 'ㄴ', '문제', '갯', '수', '보다', '는', '배워가', '는', '과정', '임', '을', '떠올리', '어', '보고', ',', '실력', '을', '더', '쌓', '아', '보', '는', '계기', '로', '삼', '아', '보', '면', '어떠', 'ㄹ까', '싶', '어요', '.', '문제', '를', '틀리', 'ㄹ', '수', '도', '있', '지', ',', '괜찮', '아', ',', '배워가', '는', '중이', '야', '하면서', '스스로', '를', '다독이', '어', '주', '고', ',', '좀', '더', '기회', '를', '주', '는', '연습', '을', '해보', '면', '좋', '겠', '어요', '.', '저', '의', '답변', '이', '조금', '이나마', '위로', '가', '되', '고', ',', '도움', '이', '되', '기', '를', '바라', 'ㄹ게요', '.', '마', 'ㄴ', '9', '세', '24', '세', '청소년', '은', '저희', '서울', '시청', '소년', '상담', '복지', '센터', '를', '이용', '하', '시', 'ㄹ', '수', '있', '으니', '더', '도움', '받', '고', '싶', '은', '주제', '가', '있', '을', '때', '언제', '든', '찾', '아', '주', '세요', '.', '서울', '시청', '소년', '상담', '복지', '센터', '사이버', '상담원', '드림']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jW7GxC0CK7s",
        "outputId": "b6cbfbd3-8112-41cc-cced-1c2425846628"
      },
      "source": [
        "# 토큰화 시키기\n",
        "%%time\n",
        "df['token']= df['counseling'].apply(kkma.morphs)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 7min 35s, sys: 2.08 s, total: 7min 37s\n",
            "Wall time: 5min 33s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7znwc5rEJux",
        "outputId": "59598dd2-ece2-4084-b186-41c2e19d3b54"
      },
      "source": [
        "df['token'].head()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    [안녕, 하세, 요, ,, 서울시, 청소년, 상담, 복지, 센터, 사이버, 상담원,...\n",
              "1    [너무, 성적, 에, 연연, 하, 지, 마시, 고, 틀리, ㄹ, 때, 마다, 약간,...\n",
              "2    [숙제, 를, 열심히, 하, ㄴ, 것, 아니, ㄹ까요, ?, 원래, 열심히, 하, ...\n",
              "3                             [학교, 상담, 하, 는, 곳, 있음, .]\n",
              "4    [상담, 자체, 로, 도, 크, ㄴ, 효과, 를, 보, ㄹ, 수, 있, 는, 경우,...\n",
              "Name: token, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHgLXXnoDbHF"
      },
      "source": [
        "# 생성된 토큰 비율 확인\n",
        "from collections import Counter\n",
        "def word_count(docs):\n",
        "    \"\"\" 토큰화된 문서들을 입력받아 토큰을 카운트 하고 관련된 속성을 가진 데이터프레임을 리턴합니다.\n",
        "    Args:\n",
        "        docs (series or list): 토큰화된 문서가 들어있는 list\n",
        "    Returns:\n",
        "        list: Dataframe\n",
        "    \"\"\"\n",
        "    # 전체 코퍼스에서 단어 빈도 카운트\n",
        "    word_counts = Counter()\n",
        "\n",
        "    # 단어가 존재하는 문서의 빈도 카운트, 단어가 한 번 이상 존재하면 +1\n",
        "    word_in_docs = Counter()\n",
        "\n",
        "    # 전체 문서의 갯수\n",
        "    total_docs = len(docs)\n",
        "\n",
        "    for doc in docs:\n",
        "        word_counts.update(doc)\n",
        "        word_in_docs.update(set(doc))\n",
        "\n",
        "    temp = zip(word_counts.keys(), word_counts.values())\n",
        "\n",
        "    wc = pd.DataFrame(temp, columns = ['word', 'count'])\n",
        "\n",
        "    # 단어의 순위\n",
        "    # method='first': 같은 값의 경우 먼저나온 요소를 우선\n",
        "    wc['rank'] = wc['count'].rank(method='first', ascending=False)\n",
        "    total = wc['count'].sum()\n",
        "\n",
        "    # 코퍼스 내 단어의 비율\n",
        "    wc['percent'] = wc['count'].apply(lambda x: x / total)\n",
        "\n",
        "    wc = wc.sort_values(by='rank')\n",
        "\n",
        "    # 누적 비율\n",
        "    # cumsum() : cumulative sum\n",
        "    wc['cul_percent'] = wc['percent'].cumsum()\n",
        "\n",
        "    temp2 = zip(word_in_docs.keys(), word_in_docs.values())\n",
        "    ac = pd.DataFrame(temp2, columns=['word', 'word_in_docs'])\n",
        "    wc = ac.merge(wc, on='word')\n",
        "    \n",
        "    # 전체 문서 중 존재하는 비율\n",
        "    wc['word_in_docs_percent'] = wc['word_in_docs'].apply(lambda x: x / total_docs)\n",
        "\n",
        "    return wc.sort_values(by='rank')"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "QWP3vuKiDh5i",
        "outputId": "228cc865-dcef-4aee-d660-7b4dec8e1afa"
      },
      "source": [
        "wc = word_count(df['token'])\n",
        "wc.head(10)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>word_in_docs</th>\n",
              "      <th>count</th>\n",
              "      <th>rank</th>\n",
              "      <th>percent</th>\n",
              "      <th>cul_percent</th>\n",
              "      <th>word_in_docs_percent</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>107</th>\n",
              "      <td>하</td>\n",
              "      <td>1018</td>\n",
              "      <td>12145</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.041722</td>\n",
              "      <td>0.041722</td>\n",
              "      <td>0.934803</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70</th>\n",
              "      <td>.</td>\n",
              "      <td>924</td>\n",
              "      <td>11039</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.037923</td>\n",
              "      <td>0.079645</td>\n",
              "      <td>0.848485</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67</th>\n",
              "      <td>이</td>\n",
              "      <td>971</td>\n",
              "      <td>8963</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.030791</td>\n",
              "      <td>0.110435</td>\n",
              "      <td>0.891644</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>157</th>\n",
              "      <td>을</td>\n",
              "      <td>894</td>\n",
              "      <td>8243</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.028317</td>\n",
              "      <td>0.138753</td>\n",
              "      <td>0.820937</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>123</th>\n",
              "      <td>는</td>\n",
              "      <td>954</td>\n",
              "      <td>6451</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.022161</td>\n",
              "      <td>0.160914</td>\n",
              "      <td>0.876033</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>166</th>\n",
              "      <td>,</td>\n",
              "      <td>738</td>\n",
              "      <td>6349</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.021811</td>\n",
              "      <td>0.182725</td>\n",
              "      <td>0.677686</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>111</th>\n",
              "      <td>고</td>\n",
              "      <td>865</td>\n",
              "      <td>6077</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0.020876</td>\n",
              "      <td>0.203602</td>\n",
              "      <td>0.794307</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>있</td>\n",
              "      <td>840</td>\n",
              "      <td>4731</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0.016253</td>\n",
              "      <td>0.219854</td>\n",
              "      <td>0.771350</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>148</th>\n",
              "      <td>ㄴ</td>\n",
              "      <td>855</td>\n",
              "      <td>4457</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0.015311</td>\n",
              "      <td>0.235165</td>\n",
              "      <td>0.785124</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>어</td>\n",
              "      <td>840</td>\n",
              "      <td>4144</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.014236</td>\n",
              "      <td>0.249401</td>\n",
              "      <td>0.771350</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    word  word_in_docs  count  ...   percent  cul_percent  word_in_docs_percent\n",
              "107    하          1018  12145  ...  0.041722     0.041722              0.934803\n",
              "70     .           924  11039  ...  0.037923     0.079645              0.848485\n",
              "67     이           971   8963  ...  0.030791     0.110435              0.891644\n",
              "157    을           894   8243  ...  0.028317     0.138753              0.820937\n",
              "123    는           954   6451  ...  0.022161     0.160914              0.876033\n",
              "166    ,           738   6349  ...  0.021811     0.182725              0.677686\n",
              "111    고           865   6077  ...  0.020876     0.203602              0.794307\n",
              "26     있           840   4731  ...  0.016253     0.219854              0.771350\n",
              "148    ㄴ           855   4457  ...  0.015311     0.235165              0.785124\n",
              "51     어           840   4144  ...  0.014236     0.249401              0.771350\n",
              "\n",
              "[10 rows x 7 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x934dJUxECL0"
      },
      "source": [
        "#불용어 없애기\n",
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize \n",
        "\n",
        "stop_words = \"아 휴 아이구 아이쿠 아이고 어 나 우리 저희 따라 의해 을 를 에 의 가 으로 로 에게 뿐이다 의거하여 근거하여 입각하여 기준으로 예하면 예를 들면 예를 들자면 저 소인 소생 저희 지말고 하지마 하지마라 다른 물론 또한 그리고 비길수 없다 해서는 안된다 뿐만 아니라 만이 아니다 만은 아니다 막론하고 관계없이 그치지 않다 그러나 그런데 하지만 든간에 논하지 않다 따지지 않다 설사 비록 더라도 아니면 만 못하다 하는 편이 낫다 불문하고 향하여 향해서 향하다 쪽으로 틈타 이용하여 타다 오르다 제외하고 이 외에 이 밖에 하여야 비로소 한다면 몰라도 외에도 이곳 여기 부터 기점으로 따라서 할 생각이다 하려고하다 이리하여 그리하여 그렇게 함으로써 하지만 일때 할때 앞에서 중에서 보는데서 으로써 로써 까지 해야한다 일것이다 반드시 할줄알다 할수있다 할수있어 임에 틀림없다 한다면 등 등등 제 겨우 단지 다만 할뿐 딩동 댕그 대해서 대하여 대하면 훨씬 얼마나 얼마만큼 얼마큼 남짓 여 얼마간 약간 다소 좀 조금 다수 몇 얼마 지만 하물며 또한 그러나 그렇지만 하지만 이외에도 대해 말하자면 뿐이다 다음에 반대로 반대로 말하자면 이와 반대로 바꾸어서 말하면 바꾸어서 한다면 만약 그렇지않으면 까악 툭 딱 삐걱거리다 보드득 비걱거리다 꽈당 응당 해야한다 에 가서 각 각각 여러분 각종 각자 제각기 하도록하다 와 과 그러므로 그래서 고로 한 까닭에 하기 때문에 거니와 이지만 대하여 관하여 관한 과연 실로 아니나다를가 생각한대로 진짜로 한적이있다 하곤하였다 하 하하 허허 아하 거바 와 오 왜 어째서 무엇때문에 어찌 하겠는가 무슨 어디 어느곳 더군다나 하물며 더욱이는 어느때 언제 야 이봐 어이 여보시오 흐흐 흥 휴 헉헉 헐떡헐떡 영차 여차 어기여차 끙끙 아야 앗 아야 콸콸 졸졸 좍좍 뚝뚝 주룩주룩 솨 우르르 그래도 또 그리고 바꾸어말하면 바꾸어말하자면 혹은 혹시 답다 및 그에 따르는 때가 되어 즉 지든지 설령 가령 하더라도 할지라도 일지라도 지든지 몇 거의 하마터면 인젠 이젠 된바에야 된이상 만큼 어찌됏든 그위에 게다가 점에서 보아 비추어 보아 고려하면 하게될것이다 일것이다 비교적 좀 보다더 비하면 시키다 하게하다 할만하다 의해서 연이서 이어서 잇따라 뒤따라 뒤이어 결국 의지하여 기대여 통하여 자마자 더욱더 불구하고 얼마든지 마음대로 주저하지 않고 곧 즉시 바로 당장 하자마자 밖에 안된다 하면된다 그래 그렇지 요컨대 다시 말하자면 바꿔 말하면 즉 구체적으로 말하자면 시작하여 시초에 이상 허 헉 허걱 바와같이 해도좋다 해도된다 게다가 더구나 하물며 와르르 팍 퍽 펄렁 동안 이래 하고있었다 이었다 에서 로부터 까지 예하면 했어요 해요 함께 같이 더불어 마저 마저도 양자 모두 습니다 가까스로 하려고하다 즈음하여 다른 다른 방면으로 해봐요 습니까 했어요 말할것도 없고 무릎쓰고 개의치않고 하는것만 못하다 하는것이 낫다 매 매번 들 모 어느것 어느 로써 갖고말하자면 어디 어느쪽 어느것 어느해 어느 년도 라 해도 언젠가 어떤것 어느것 저기 저쪽 저것 그때 그럼 그러면 요만한걸 그래 그때 저것만큼 그저 이르기까지 할 줄 안다 할 힘이 있다 너 너희 당신 어찌 설마 차라리 할지언정 할지라도 할망정 할지언정 구토하다 게우다 토하다 메쓰겁다 옆사람 퉤 쳇 의거하여 근거하여 의해 따라 힘입어 그 다음 버금 두번째로 기타 첫번째로 나머지는 그중에서 견지에서 형식으로 쓰여 입장에서 위해서 단지 의해되다 하도록시키다 뿐만아니라 반대로 전후 전자 앞의것 잠시 잠깐 하면서 그렇지만 다음에 그러한즉 그런즉 남들 아무거나 어찌하든지 같다 비슷하다 예컨대 이럴정도로 어떻게 만약 만일 위에서 서술한바와같이 인 듯하다 하지 않는다면 만약에 무엇 무슨 어느 어떤 아래윗 조차 한데 그럼에도 불구하고 여전히 심지어 까지도 조차도 하지 않도록 않기 위하여 때 시각 무렵 시간 동안 어때 어떠한 하여금 네 예 우선 누구 누가 알겠는가 아무도 줄은모른다 줄은 몰랏다 하는 김에 겸사겸사 하는바 그런 까닭에 한 이유는 그러니 그러니까 때문에 그 너희 그들 너희들 타인 것 것들 너 위하여 공동으로 동시에 하기 위하여 어찌하여 무엇때문에 붕붕 윙윙 나 우리 엉엉 휘익 윙윙 오호 아하 어쨋든 만 못하다 하기보다는 차라리 하는 편이 낫다 흐흐 놀라다 상대적으로 말하자면 마치 아니라면 쉿 그렇지 않으면 그렇지 않다면 안 그러면 아니었다면 하든지 아니면 이라면 좋아 알았어 하는것도 그만이다 어쩔수 없다 하나 일 일반적으로 일단 한켠으로는 오자마자 이렇게되면 이와같다면 전부 한마디 한항목 근거로 하기에 아울러 하지 않도록 않기 위해서 이르기까지 이 되다 로 인하여 까닭으로 이유만으로 이로 인하여 그래서 이 때문에 그러므로 그런 까닭에 알 수 있다 결론을 낼 수 있다 으로 인하여 있다 어떤것 관계가 있다 관련이 있다 연관되다 어떤것들 에 대해 이리하여 그리하여 여부 하기보다는 하느니 하면 할수록 운운 이러이러하다 하구나 하도다 다시말하면 다음으로 에 있다 에 달려 있다 우리 우리들 오히려 하기는한데 어떻게 어떻해 어찌됏어 어때 어째서 본대로 자 이 이쪽 여기 이것 이번 이렇게말하자면 이런 이러한 이와 같은 요만큼 요만한 것 얼마 안 되는 것 이만큼 이 정도의 이렇게 많은 것 이와 같다 이때 이렇구나 것과 같이 끼익 삐걱 따위 와 같은 사람들 부류의 사람들 왜냐하면 중의하나 오직 오로지 에 한하다 하기만 하면 도착하다 까지 미치다 도달하다 정도에 이르다 할 지경이다 결과에 이르다 관해서는 여러분 하고 있다 한 후 혼자 자기 자기집 자신 우에 종합한것과같이 총적으로 보면 총적으로 말하면 총적으로 대로 하다 으로서 참 그만이다 할 따름이다 쿵 탕탕 쾅쾅 둥둥 봐 봐라 아이야 아니 와아 응 아이 참나 년 월 일 령 영 일 이 삼 사 오 육 륙 칠 팔 구 이천육 이천칠 이천팔 이천구 하나 둘 셋 넷 다섯 여섯 일곱 여덟 아홉 령 영 이 있 하 것 들 그 되 수 이 보 않 없 나 사람 주 아니 등 같 우리 때 년 가 한 지 대하 오 말 일 그렇 위하 때문 그것 두 말하 알 그러나 받 못하 일 그런 또 문제 더 사회 많 그리고 좋 크 따르 중 나오 가지 씨 시키 만들 지금 생각하 그러 속 하나 집 살 모르 적 월 데 자신 안 어떤 내 내 경우 명 생각 시간 그녀 다시 이런 앞 보이 번 나 다른 어떻 여자 개 전 들 사실 이렇 점 싶 말 정도 좀 원 잘 통하 놓 , .\"\n",
        "\n",
        "word_tokens = df['token']\n",
        "stop_words=stop_words.split(' ')\n",
        "\n",
        "result = []\n",
        "\n",
        "for doc in word_tokens:\n",
        "    \n",
        "    doc_tokens = []\n",
        "\n",
        "    for w in doc: \n",
        "        if w not in stop_words: \n",
        "            doc_tokens.append(w) \n",
        "\n",
        "    result.append(doc_tokens)\n",
        "\n",
        "df['token_1'] = result"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37D9iui-HleS",
        "outputId": "b6e52b1e-0c70-42eb-949a-3cd124655c10"
      },
      "source": [
        "df['token'].head()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    [안녕, 하세, 요, ,, 서울시, 청소년, 상담, 복지, 센터, 사이버, 상담원,...\n",
              "1    [너무, 성적, 에, 연연, 하, 지, 마시, 고, 틀리, ㄹ, 때, 마다, 약간,...\n",
              "2    [숙제, 를, 열심히, 하, ㄴ, 것, 아니, ㄹ까요, ?, 원래, 열심히, 하, ...\n",
              "3                             [학교, 상담, 하, 는, 곳, 있음, .]\n",
              "4    [상담, 자체, 로, 도, 크, ㄴ, 효과, 를, 보, ㄹ, 수, 있, 는, 경우,...\n",
              "Name: token, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djI-2T1BHhF1",
        "outputId": "a48729f6-238d-43da-fc9a-6f32a8a52fa5"
      },
      "source": [
        "df['token_1'].head()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    [안녕, 하세, 요, 서울시, 청소년, 상담, 복지, 센터, 사이버, 상담원, ㅂ니...\n",
              "1    [너무, 성적, 연연, 마시, 고, 틀리, ㄹ, 마다, 완벽, 게, 어서, 목표, ...\n",
              "2    [숙제, 열심히, ㄴ, ㄹ까요, ?, 원래, 열심히, ㄹ, 수록, 잘못되, 면, 짜...\n",
              "3                                   [학교, 상담, 는, 곳, 있음]\n",
              "4    [상담, 자체, 도, ㄴ, 효과, ㄹ, 는, 도, 기에, 학생, 시, 라면, 학, ...\n",
              "Name: token_1, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "dwlsyUj4HtK-",
        "outputId": "acadb81e-3872-4784-b77d-c7a1c80f3818"
      },
      "source": [
        "wc = word_count(df['token_1'])\n",
        "wc.head(10)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>word_in_docs</th>\n",
              "      <th>count</th>\n",
              "      <th>rank</th>\n",
              "      <th>percent</th>\n",
              "      <th>cul_percent</th>\n",
              "      <th>word_in_docs_percent</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>91</th>\n",
              "      <td>는</td>\n",
              "      <td>954</td>\n",
              "      <td>6451</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.037844</td>\n",
              "      <td>0.037844</td>\n",
              "      <td>0.876033</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84</th>\n",
              "      <td>고</td>\n",
              "      <td>865</td>\n",
              "      <td>6077</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.035650</td>\n",
              "      <td>0.073493</td>\n",
              "      <td>0.794307</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>107</th>\n",
              "      <td>ㄴ</td>\n",
              "      <td>855</td>\n",
              "      <td>4457</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.026146</td>\n",
              "      <td>0.099639</td>\n",
              "      <td>0.785124</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>상담</td>\n",
              "      <td>646</td>\n",
              "      <td>3755</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.022028</td>\n",
              "      <td>0.121667</td>\n",
              "      <td>0.593205</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>116</th>\n",
              "      <td>ㄹ</td>\n",
              "      <td>839</td>\n",
              "      <td>3728</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.021870</td>\n",
              "      <td>0.143537</td>\n",
              "      <td>0.770432</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>122</th>\n",
              "      <td>은</td>\n",
              "      <td>812</td>\n",
              "      <td>3136</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.018397</td>\n",
              "      <td>0.161934</td>\n",
              "      <td>0.745638</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>어요</td>\n",
              "      <td>609</td>\n",
              "      <td>2453</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0.014390</td>\n",
              "      <td>0.176324</td>\n",
              "      <td>0.559229</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>시</td>\n",
              "      <td>742</td>\n",
              "      <td>2240</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0.013141</td>\n",
              "      <td>0.189464</td>\n",
              "      <td>0.681359</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>103</th>\n",
              "      <td>도</td>\n",
              "      <td>711</td>\n",
              "      <td>2070</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0.012143</td>\n",
              "      <td>0.201607</td>\n",
              "      <td>0.652893</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>113</th>\n",
              "      <td>ㅂ니다</td>\n",
              "      <td>875</td>\n",
              "      <td>2039</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.011961</td>\n",
              "      <td>0.213569</td>\n",
              "      <td>0.803489</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    word  word_in_docs  count  ...   percent  cul_percent  word_in_docs_percent\n",
              "91     는           954   6451  ...  0.037844     0.037844              0.876033\n",
              "84     고           865   6077  ...  0.035650     0.073493              0.794307\n",
              "107    ㄴ           855   4457  ...  0.026146     0.099639              0.785124\n",
              "40    상담           646   3755  ...  0.022028     0.121667              0.593205\n",
              "116    ㄹ           839   3728  ...  0.021870     0.143537              0.770432\n",
              "122    은           812   3136  ...  0.018397     0.161934              0.745638\n",
              "41    어요           609   2453  ...  0.014390     0.176324              0.559229\n",
              "49     시           742   2240  ...  0.013141     0.189464              0.681359\n",
              "103    도           711   2070  ...  0.012143     0.201607              0.652893\n",
              "113  ㅂ니다           875   2039  ...  0.011961     0.213569              0.803489\n",
              "\n",
              "[10 rows x 7 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9Z9I9tKKerA",
        "outputId": "1b3c9851-5add-4856-d58b-cfdec8e6458c"
      },
      "source": [
        "!pip install gensim --upgrade"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n",
            "Collecting gensim\n",
            "  Downloading gensim-4.1.2-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (24.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 24.1 MB 2.1 kB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.2.1)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.19.5)\n",
            "Installing collected packages: gensim\n",
            "  Attempting uninstall: gensim\n",
            "    Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "Successfully installed gensim-4.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "id": "wjpTMiPyKxSC",
        "outputId": "c4338996-ff36-447d-cd7a-bbbfefbff532"
      },
      "source": [
        "#토큰 길이 확인하기\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print('토큰의 최대 길이 :',max(len(l) for l in df['token_1']))\n",
        "print('토큰의 평균 길이 :',sum(map(len, df['token_1']))/len(df['token_1']))\n",
        "plt.hist([len(s) for s in df['token_1']], bins=50)\n",
        "plt.xlabel('length of token')\n",
        "plt.ylabel('number of token')\n",
        "plt.show()"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "토큰의 최대 길이 : 4233\n",
            "토큰의 평균 길이 : 156.53351698806244\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0T1zH7-tZeXd"
      },
      "source": [
        "- 데이터 임베딩 ( word2vec이냐 fasttext이냐 )"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sM8cyIySEEOH"
      },
      "source": [
        "#word2vec적용하기\n",
        "#from gensim.models import Word2Vec\n",
        "#model = Word2Vec(sentences = df['token_1'], size = 400, window = 5, min_count = 10, workers = 4, sg = 0)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NTpOUCP8jIaH",
        "outputId": "c7fca6e2-9c97-4f05-e4cf-b0b228089b78"
      },
      "source": [
        "pip install jamo"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jamo\n",
            "  Downloading jamo-0.4.1-py3-none-any.whl (9.5 kB)\n",
            "Installing collected packages: jamo\n",
            "Successfully installed jamo-0.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "6FvcX-3lt4ZW",
        "outputId": "57b63228-9dad-4adb-9369-ea513446eea5"
      },
      "source": [
        "%pwd"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/My Drive/코드스테이츠 과제/project4'"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWeuf7m4t_qL",
        "outputId": "f7f7d395-6805-4c88-d28c-73b92bcf063c"
      },
      "source": [
        "%cd /content/drive/MyDrive/코드스테이츠 과제/project4"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/코드스테이츠 과제/project4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-FGNh73XgjpD",
        "outputId": "3ff8b3ed-89d9-400f-873e-c81e4aafdfc3"
      },
      "source": [
        "from jamo import h2j, j2hcj \n",
        "from unicode import join_jamos \n",
        "\n",
        "text = \"동해물과 백두산이 마르고 닳도록\" \n",
        "jamo_str = j2hcj(h2j(text)) \n",
        "print(jamo_str) \n",
        "\n",
        "merge_jamo = join_jamos(jamo_str) \n",
        "print(merge_jamo)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ㄷㅗㅇㅎㅐㅁㅜㄹㄱㅘ ㅂㅐㄱㄷㅜㅅㅏㄴㅇㅣ ㅁㅏㄹㅡㄱㅗ ㄷㅏㅀㄷㅗㄹㅗㄱ\n",
            "동해물과 백두산이 마르고 닳도록\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-OooT7yXBxp"
      },
      "source": [
        "df['token_1'][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4XSq0JpyHGR"
      },
      "source": [
        "word_tokens = df['token_1']\n",
        "\n",
        "corpus = []\n",
        "\n",
        "for doc in word_tokens:\n",
        "    \n",
        "    corpus_tokens = []\n",
        "\n",
        "    for w in doc: \n",
        "        w = j2hcj(h2j(w)) \n",
        "        corpus_tokens.append(w)\n",
        "\n",
        "    corpus.append(corpus_tokens)\n",
        "\n",
        "df['token_corpus'] = corpus"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYX_nIw0xJIx"
      },
      "source": [
        "df['token_corpus'][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "id": "_7fE1TjP1GqB",
        "outputId": "cf52901d-fb31-49d5-bcd7-039d4e218d5f"
      },
      "source": [
        "print('토큰의 최대 길이 :',max(len(l) for l in df['token_corpus']))\n",
        "print('토큰의 평균 길이 :',sum(map(len, df['token_corpus']))/len(df['token_corpus']))\n",
        "plt.hist([len(s) for s in df['token_corpus']], bins=50)\n",
        "plt.xlabel('length of token')\n",
        "plt.ylabel('number of token')\n",
        "plt.show()"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "토큰의 최대 길이 : 4233\n",
            "토큰의 평균 길이 : 156.53351698806244\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEGCAYAAACKB4k+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAZeUlEQVR4nO3de7RedX3n8fenKYKjVkSOrJiAwRrHwV4iRqRL27FQFaEjuOoFahWVNr1g1WJbQ52plzXO4NSKOrW2WNTgqIBUC4N2FBGqTgGbSLiJ1MhlSSaSqIBQV6nE7/yxf2f7eDiXJyHP8yQ579dazzp7//bt+/zWOud79m//9u+XqkKSJICfmHQAkqTdh0lBktQzKUiSeiYFSVLPpCBJ6v3kpAN4MA488MBasWLFpMOQpD3Khg0bvl1VU7Nt26OTwooVK1i/fv2kw5CkPUqS2+baZvORJKlnUpAk9UwKkqTeyJNCkiVJrk5ycVs/NMlVSTYlOS/JQ1r5vm19U9u+YtSxSZJ+3DjuFF4L3Diw/nbgzKp6AnAncEorPwW4s5Wf2faTJI3RSJNCkuXAccDftPUARwEXtF3WASe05ePbOm370W1/SdKYjPpO4V3AHwM/bOuPBu6qqvvb+u3Asra8DPgmQNt+d9tfkjQmI0sKSX4V2FpVG3bxedckWZ9k/bZt23blqSVp0RvlncIzgOcnuRU4l67Z6N3A/kmmX5pbDmxuy5uBgwHa9kcC35l50qo6q6pWV9XqqalZX8iTJO2kkb3RXFWnA6cDJHkW8IdV9dIkHwdeSJcoTgYubIdc1NavaNs/XyOcAWjF2k/Nue3WM44b1WUlabc2ifcU3gCclmQT3TODs1v52cCjW/lpwNoJxCZJi9pYxj6qqsuBy9vyzcARs+zzr8CLxhGPJGl2vtEsSeqZFCRJPZOCJKlnUpAk9UwKkqSeSUGS1DMpSJJ6JgVJUs+kIEnqmRQkST2TgiSpZ1KQJPVMCpKknklBktQzKUiSeiYFSVLPpCBJ6pkUJEm9kSWFJPsl+XKSa5LckOQtrfxDSW5JsrF9VrXyJHlPkk1Jrk1y+KhikyTNbpRzNN8HHFVV9ybZB/hSkr9v2/6oqi6Ysf/zgJXt83Tgfe2nJGlMRnanUJ172+o+7VPzHHI8cE477kpg/yRLRxWfJOmBRvpMIcmSJBuBrcAlVXVV2/S21kR0ZpJ9W9ky4JsDh9/eymaec02S9UnWb9u2bZThS9KiM9KkUFXbq2oVsBw4IsnPAKcDTwKeBhwAvGEHz3lWVa2uqtVTU1O7PGZJWszG0vuoqu4CLgOOqaotrYnoPuCDwBFtt83AwQOHLW9lkqQxGWXvo6kk+7flhwLPBr42/ZwgSYATgOvbIRcBL2+9kI4E7q6qLaOKT5L0QKPsfbQUWJdkCV3yOb+qLk7y+SRTQICNwO+0/T8NHAtsAr4PvHKEsUmSZjGypFBV1wJPmaX8qDn2L+DUUcUjSVqYbzRLknomBUlSz6QgSeqZFCRJPZOCJKlnUpAk9UwKkqSeSUGS1DMpSJJ6JgVJUs+kIEnqmRQkST2TgiSpZ1KQJPVMCpKknklBktQzKUiSeiObeS3JfsAXgH3bdS6oqjclORQ4F3g0sAF4WVX9W5J9gXOApwLfAV5SVbeOKr6dsWLtp2Ytv/WM48YciSSNxijvFO4DjqqqnwdWAcckORJ4O3BmVT0BuBM4pe1/CnBnKz+z7SdJGqORJYXq3NtW92mfAo4CLmjl64AT2vLxbZ22/egkGVV8kqQHGukzhSRLkmwEtgKXAN8A7qqq+9sutwPL2vIy4JsAbfvddE1MM8+5Jsn6JOu3bds2yvAladEZaVKoqu1VtQpYDhwBPGkXnPOsqlpdVaunpqYedIySpB8ZS++jqroLuAz4BWD/JNMPuJcDm9vyZuBggLb9kXQPnCVJYzKypJBkKsn+bfmhwLOBG+mSwwvbbicDF7bli9o6bfvnq6pGFZ8k6YFG1iUVWAqsS7KELvmcX1UXJ/kqcG6S/wpcDZzd9j8b+HCSTcB3gRNHGJskaRYjSwpVdS3wlFnKb6Z7vjCz/F+BF40qHknSwnyjWZLUMylIknomBUlSb8FnCkmmgN8CVgzuX1WvGl1YkqRJGOZB84XAF4HPAdtHG44kaZKGSQr/rqreMPJIJEkTN8wzhYuTHDvySCRJEzdMUngtXWL41yTfS3JPku+NOjBJ0vgt2HxUVY8YRyCSpMlb8E4hnd9I8l/a+sFJHvBGsiRpzzdM89Ff0o1u+utt/V7gvSOLSJI0McP0Pnp6VR2e5GqAqrozyUNGHJckaQKGuVP4QRvptKB/me2HI41KkjQRwySF9wCfBB6T5G3Al4D/NtKoJEkTMUzz0QXABuBoIMAJwB2jDEqSNBnDJIVPACdU1dcAkiwFLgGeOsrAJEnjN0zz0d8B5ydZkmQF8Bng9FEGJUmajAWTQlW9n24wvL8D/jfwO1X12YWOa+8zXJbkq0luSPLaVv7mJJuTbGyfYweOOT3JpiQ3JXnuzn8tSdLOmLP5KMlpg6vAIcBG4MgkR1bVOxc49/3A66vqK0keAWxIcknbdmZVvWPG9Q6jm5f5ycBjgc8leWJVOTKrJI3JfM8UZg5v8Yk5ymdVVVuALW35niQ3AsvmOeR44Nyqug+4JckmurmcrxjmepKkB2/OpFBVbxlcT/LwVn7vjl6kPYt4CnAV8Azg1UleDqynu5u4ky5hXDlw2O3MkkSSrAHWABxyyCE7GookaR7DjH30M+1t5huAG5JsSPLkYS/QksnfAq+rqu8B7wN+GlhFdyfx5zsScFWdVVWrq2r11NTUjhwqSVrAML2PzgJOq6rHVdXjgNcD7x/m5En2oUsIH6mqTwBU1R1Vtb2qftjOMz243mbg4IHDl7cySdKYDJMUHlZVl02vVNXlwMMWOihJgLOBGwcfSrf3HKa9ALi+LV8EnJhk3ySHAiuBLw8RnyRpFxnm5bWb27DZH27rvwHcPMRxzwBeBlyXZGMr+xPgpCSr6MZSuhX4bYCquiHJ+cBX6XounWrPI0kar2GSwquAt9D1Pirgi8ArFzqoqr5E15V1pk/Pc8zbgLcNEZMkaQSGSQq/UlWvGSxI8iLg46MJSZI0KcM8U5htSAuHuZCkvdB8bzQ/DzgWWJbkPQObfoquzV+StJeZr/no/9G9XPZ8uqGzp90D/MEog5IkTcZ8bzRfA1yT5KNV9YMxxiRJmpBhRkk1IUjSIjHMg2ZJ0iIxZ1JI8uH287XjC0eSNEnz3Sk8NcljgVcleVSSAwY/4wpQkjQ+8/U++ivgUuDxdL2PBt9OrlYuSdqLzHmnUFXvqar/AHygqh5fVYcOfEwIkrQXWnCYi6r63SQ/D/xiK/pCVV072rAkSZMwzCQ7rwE+AjymfT6S5PdHHZgkafyGGRDvN4GnV9W/ACR5O928yf9zlIFJksZvmPcUAgzOa7Cd2YfEliTt4Ya5U/ggcFWST7b1E+hmVJMk7WWGedD8ziSXA89sRa+sqqtHGtWErVj7qUmHIEkTMcydAlX1FeArO3LiJAcD5wAH0b3XcFZVvbu9+HYesIJuOs4XV9WdbU7nd9MN1/194BXtupKkMRnl2Ef3A6+vqsOAI4FTkxwGrAUuraqVdC/HrW37Pw9Y2T5rgPeNMDZJ0ixGlhSqasv0f/pVdQ9wI7AMOB5Y13ZbR/eMglZ+TnWuBPZPsnRU8UmSHmjepJBkSZLLHuxFkqwAngJcBRxUVVvapm/RNS9BlzC+OXDY7a1s5rnWJFmfZP22bdsebGiSpAHzJoWq2g78MMkjd/YCSR4O/C3wuqr63ozzF93zhqFV1VlVtbqqVk9NTe1sWJKkWQzzoPle4LoklwD/Ml1YVa9Z6MAk+9AlhI9U1Sda8R1JllbVltY8tLWVbwYOHjh8eSuTJI3JMEnhE+2zQ1pvorOBG6vqnQObLgJOBs5oPy8cKH91knOBpwN3DzQzSZLGYJj3FNYleShwSFXdtAPnfgbwMrq7jI2t7E/oksH5SU4BbgNe3LZ9mq476ia6Lqmv3IFrSZJ2gQWTQpL/BLwDeAhwaJJVwFur6vnzHVdVX2Lu4TCOnmX/Ak5dMGJJ0sgM0yX1zcARwF0AVbURJ9iRpL3SMEnhB1V194yyH44iGEnSZA3zoPmGJL8OLEmyEngN8I+jDUuSNAnD3Cn8PvBk4D7gY8D3gNeNMihJ0mQM0/vo+8Ab2+Q61YaskCTthYaZjvNpSa4DrqXrXnpNkqeOPjRJ0rgN80zhbOD3quqLAEmeSTfxzs+NMjBJ0vgN80xh+3RCgP79g/tHF5IkaVLmvFNIcnhb/Ickf033kLmAlwCXjz40SdK4zdd89Ocz1t80sLxDI5tKkvYMcyaFqvrlcQYiSZq8YcY+2h94Od2cyv3+wwydLUnaswzT++jTwJXAdTi8hSTt1YZJCvtV1Wkjj0SSNHHDdEn9cJLfSrI0yQHTn5FHJkkau2HuFP4N+DPgjfyo11Hh8NmStNcZJim8HnhCVX171MFIkiZrmOaj6ekxd0iSDyTZmuT6gbI3J9mcZGP7HDuw7fQkm5LclOS5O3o9SdKDN8ydwr8AG5NcRjd8NjBUl9QPAX8BnDOj/MyqesdgQZLDgBPphuh+LPC5JE+squ1DxCdJ2kWGSQp/1z47pKq+kGTFkLsfD5xbVfcBtyTZRDcF6BU7el1J0s4bZj6Fdbv4mq9O8nJgPfD6qroTWEb3LsS021vZAyRZA6wBOOSQQ3ZxaJK0uA0zn8ItSW6e+dnJ670P+GlgFbCFB46vtKCqOquqVlfV6qmpqZ0MQ5I0m2Gaj1YPLO8HvAjYqfcUquqO6eUk7wcubqubgYMHdl3eyiRJY7TgnUJVfWfgs7mq3gUctzMXS7J0YPUFwHTPpIuAE5Psm+RQYCXw5Z25hiRp5w0zIN7hA6s/QXfnMMxxHwOeBRyY5Ha6obeflWQV3ctvtwK/DVBVNyQ5H/gq3QQ+p9rzSJLGb5jmo8F2//vp/pi/eKGDquqkWYrPnmf/twFvGyIeSdKIDNP7yHkVJGmRGKYZaF/g13jgfApvHV1Ye5YVaz81a/mtZ+zUoxdJmphhmo8uBO4GNjDwRrMkae8zTFJYXlXHjDwSSdLEDTMg3j8m+dmRRyJJmrhh7hSeCbwiyS10zUcBqqp+bqSRSZLGbpik8LyRRyFJ2i0M0yX1tnEEIkmavGGeKUiSFgmTgiSpZ1KQJPVMCpKknklBktQzKUiSeiYFSVLPpCBJ6pkUJEm9kSWFJB9IsjXJ9QNlByS5JMnX289HtfIkeU+STUmunTEFqCRpTEZ5p/AhYOaQ22uBS6tqJXBpW4dufKWV7bMGeN8I45IkzWFkSaGqvgB8d0bx8cC6trwOOGGg/JzqXAnsn2TpqGKTJM1u3M8UDqqqLW35W8BBbXkZ8M2B/W5vZZKkMZrYg+aqKqB29Lgka5KsT7J+27ZtI4hMkhavcSeFO6abhdrPra18M3DwwH7LW9kDVNVZVbW6qlZPTU2NNFhJWmzGnRQuAk5uyycDFw6Uv7z1QjoSuHugmUmSNCbDzLy2U5J8DHgWcGCS24E3AWcA5yc5BbgNeHHb/dPAscAm4PvAK0cVlyRpbiNLClV10hybjp5l3wJOHVUskqTh+EazJKlnUpAk9UwKkqSeSUGS1DMpSJJ6JgVJUs+kIEnqmRQkST2TgiSpZ1KQJPVMCpKknklBktQzKUiSeiYFSVLPpCBJ6pkUJEk9k4IkqTeymdfmk+RW4B5gO3B/Va1OcgBwHrACuBV4cVXdOYn4JGmxmuSdwi9X1aqqWt3W1wKXVtVK4NK2Lkkao92p+eh4YF1bXgecMMFYJGlRmlRSKOCzSTYkWdPKDqqqLW35W8BBkwlNkhaviTxTAJ5ZVZuTPAa4JMnXBjdWVSWp2Q5sSWQNwCGHHDL6SCVpEZnInUJVbW4/twKfBI4A7kiyFKD93DrHsWdV1eqqWj01NTWukCVpURh7UkjysCSPmF4GngNcD1wEnNx2Oxm4cNyxSdJiN4nmo4OATyaZvv5Hq+r/JPkn4PwkpwC3AS+eQGyStKiNPSlU1c3Az89S/h3g6HHHI0n6kd2pS6okacJMCpKknklBktQzKUiSeiYFSVLPpCBJ6pkUJEk9k4IkqWdSkCT1TAqSpJ5JQZLUMylIknqTmmRnUVix9lOzlt96xnFjjkSShuOdgiSpZ1KQJPVMCpKknklBktTzQfMEzPUAei4+mJY0LrtdUkhyDPBuYAnwN1V1xoRD2m3Zu0nSrrZbNR8lWQK8F3gecBhwUpLDJhuVJC0eu9udwhHApqq6GSDJucDxwFcnGtWE7Whz0666g5jvurvqXN7VSPMb9+9OqmokJ94ZSV4IHFNVv9nWXwY8vapePbDPGmBNW/33wE07ebkDgW8/iHD3dtbP/Kyf+Vk/85t0/TyuqqZm27C73SksqKrOAs56sOdJsr6qVu+CkPZK1s/8rJ/5WT/z253rZ7d6pgBsBg4eWF/eyiRJY7C7JYV/AlYmOTTJQ4ATgYsmHJMkLRq7VfNRVd2f5NXAZ+i6pH6gqm4Y0eUedBPUXs76mZ/1Mz/rZ367bf3sVg+aJUmTtbs1H0mSJsikIEnqLcqkkOSYJDcl2ZRk7aTjGZckH0iyNcn1A2UHJLkkydfbz0e18iR5T6uja5McPnDMyW3/ryc5eRLfZVdLcnCSy5J8NckNSV7byq0fIMl+Sb6c5JpWP29p5YcmuarVw3mtgwhJ9m3rm9r2FQPnOr2V35TkuZP5RqORZEmSq5Nc3Nb3vPqpqkX1oXuA/Q3g8cBDgGuAwyYd15i++y8BhwPXD5T9D2BtW14LvL0tHwv8PRDgSOCqVn4AcHP7+ai2/KhJf7ddUDdLgcPb8iOAf6YbasX66b5XgIe35X2Aq9r3Ph84sZX/FfC7bfn3gL9qyycC57Xlw9rv3L7Aoe13ccmkv98urKfTgI8CF7f1Pa5+FuOdQj+URlX9GzA9lMZer6q+AHx3RvHxwLq2vA44YaD8nOpcCeyfZCnwXOCSqvpuVd0JXAIcM/roR6uqtlTVV9ryPcCNwDKsHwDa97y3re7TPgUcBVzQymfWz3S9XQAcnSSt/Nyquq+qbgE20f1O7vGSLAeOA/6mrYc9sH4WY1JYBnxzYP32VrZYHVRVW9ryt4CD2vJc9bTX11+7lX8K3X/D1k/TmkY2Alvpkt03gLuq6v62y+B37euhbb8beDR7cf0A7wL+GPhhW380e2D9LMakoDlUd/+6qPsoJ3k48LfA66rqe4PbFnv9VNX2qlpFN9LAEcCTJhzSbiPJrwJbq2rDpGN5sBZjUnAojR93R2v2oP3c2srnqqe9tv6S7EOXED5SVZ9oxdbPDFV1F3AZ8At0zWbTL8EOfte+Htr2RwLfYe+tn2cAz09yK12T9FF088LscfWzGJOCQ2n8uIuA6R4yJwMXDpS/vPWyORK4uzWjfAZ4TpJHtZ44z2lle7TWnns2cGNVvXNgk/UDJJlKsn9bfijwbLrnLpcBL2y7zayf6Xp7IfD5dqd1EXBi631zKLAS+PJ4vsXoVNXpVbW8qlbQ/U35fFW9lD2xfib9tH4SH7qeI/9M1yb6xknHM8bv/TFgC/ADurbKU+jaMS8Fvg58Djig7Ru6CY++AVwHrB44z6voHoBtAl456e+1i+rmmXRNQ9cCG9vnWOun/04/B1zd6ud64E9b+ePp/mhtAj4O7NvK92vrm9r2xw+c642t3m4Cnjfp7zaCunoWP+p9tMfVj8NcSJJ6i7H5SJI0B5OCJKlnUpAk9UwKkqSeSUGS1DMpaK+Q5N6F99rhc65KcuzA+puT/OGDON+LktyY5LIZ5SuS/PoQx78iyV/s7PWlYZgUpLmtontXYVc5BfitqvrlGeUrgAWTgjQOJgXtdZL8UZJ/avMcTI/7v6L9l/7+Nh/AZ9ubuSR5Wtt3Y5I/S3J9e9v9rcBLWvlL2ukPS3J5kpuTvGaO65+U5Lp2nre3sj+le0Hu7CR/NuOQM4BfbNf5g3RzF3ywnePqJDOTCEmOS3JFkgOTPKctfyXJx9v4TSS5NclbWvl1SRyrSAsyKWivkuQ5dEMDHEH3n/5Tk/xS27wSeG9VPRm4C/i1Vv5B4LerG+xtO0B1w6r/Kd0496uq6ry275Pohsc+AnhTGy9p8PqPBd5ON/bNKuBpSU6oqrcC64GXVtUfzQh7LfDFdp0zgVO7EOpngZOAdUn2G7jGC9ox03cx/xn4lao6vF3jtIFzf7uVvw/Y6aYvLR4mBe1tntM+VwNfofsjvrJtu6WqNrblDcCKNp7PI6rqilb+0QXO/6nqxrr/Nt3geAfN2P404PKq2lbdkMgfoZvcaEc8E/hfAFX1NeA24Ilt21HAG4Djqpuv4Ui6iVn+bxvW+mTgcQPnmh7YbwNdM5U0r59ceBdpjxLgv1fVX/9YYTdHwn0DRduBh+7E+WeeY9y/Q9OzBj6R7q4gdJP6nDTH/tPxTiJW7YG8U9De5jPAqwba1ZclecxcO1c3DPQ9SZ7eik4c2HwP3dScO+LLwH9sbf1L6Jp//mGBY2Ze54vAS1v8TwQOoRscDbq7hl8DzknyZOBK4BlJntD2f1g7RtopJgXtVarqs3RNQFckuY5uqsOF/rCfAry/Nb88jG4WLOiGPT5sxoPmha6/ha69/zK6uXY3VNWF8x/FtcD2JNck+QPgL4GfaPGfB7yiqvo7lNak9FK6UTZ/CngF8LEk1wJX4OQ3ehAcJVWLXpKHV5t/OMlaYGlVvXbCYUkTYRujBMclOZ3u9+E2uv+8pUXJOwVJUs9nCpKknklBktQzKUiSeiYFSVLPpCBJ6v1/xjUM4QFDybkAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yB1QspnDZcPw",
        "outputId": "797aa314-8461-4847-da12-52630c398f7f"
      },
      "source": [
        "#fasttext 적용하기  \n",
        "from gensim.models import FastText\n",
        "model = FastText(df['token_corpus'], vector_size=50, workers=4, sg=1, word_ngrams=1, min_count =3) #iter=2,\n",
        "print(f\"학습 소요 시간 : {model.total_train_time}\")"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "학습 소요 시간 : 8.631763424999008\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQ8ocmfIhd7D"
      },
      "source": [
        "#fasttext vocab만들기\n",
        "model.build_vocab(df['token_corpus'])"
      ],
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6nHlC9PiyNRm",
        "outputId": "6d048975-cbfc-43fd-829f-acd9652b8f3b"
      },
      "source": [
        "#fasttext 훈련시키기\n",
        "model.train(\n",
        "    df['token_corpus'], epochs=model.epochs,\n",
        "    total_examples=model.corpus_count, total_words=model.corpus_total_words,\n",
        ")"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(622127, 852325)"
            ]
          },
          "metadata": {},
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rcaE6sa5YYi"
      },
      "source": [
        "def transform(list): \n",
        "  return [(join_jamos(w), r) for (w, r) in list]"
      ],
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HUMLJ2sI47Ha",
        "outputId": "7bde1eb6-8034-4502-85e0-17d6d89e267c"
      },
      "source": [
        "print(transform(model.wv.most_similar(j2hcj(h2j('친구')), topn=5)))\n",
        "print(transform(model.wv.most_similar(j2hcj(h2j('고민')), topn=5)))\n",
        "print(transform(model.wv.most_similar(j2hcj(h2j('엄마')), topn=5)))\n",
        "print(transform(model.wv.most_similar(j2hcj(h2j('진로')), topn=5)))"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('친하', 0.9386789202690125), ('친', 0.9297067523002625), ('옆', 0.9081870913505554), ('걸', 0.9076166152954102), ('만남', 0.9046932458877563)]\n",
            "[('힘들', 0.8791881799697876), ('고맙', 0.8772603273391724), ('엄마', 0.869956910610199), ('끌', 0.8622254729270935), ('터놓', 0.8609519600868225)]\n",
            "[('엄두', 0.9541079998016357), ('언제나', 0.9497414827346802), ('어플', 0.9474387168884277), ('어머니', 0.9464944005012512), ('어느새', 0.9448943138122559)]\n",
            "[('진리', 0.8584847450256348), ('진화', 0.8279927372932434), ('지니', 0.8151033520698547), ('입학제', 0.8125683665275574), ('진료', 0.8096590042114258)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8c2VjnDp5b72",
        "outputId": "2e152cdf-44eb-464b-c3b7-3cfd520d715f"
      },
      "source": [
        "df['token_corpus'].shape"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1089,)"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7ZGVi4kiaCf"
      },
      "source": [
        "ft_word_to_index = model.wv.key_to_index\n",
        "ft_index_to_word = dict([(value, key) for (key, value) in ft_word_to_index.items()])"
      ],
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_7StVrUaMDh"
      },
      "source": [
        "# 의문점.. 인코딩을 따로 안해도 되는 것인가..? 해야할 거 같은데 자소분리된 문자를 인코딩해도 되는 건지 고민이 필요하다.\n",
        "word_to_index={}\n",
        "index_to_word={}\n",
        "for word_encoder in df['token_corpus'] :\n",
        "  cnt=Counter(word_encoder)\n",
        "\n",
        "# 빈도수 기준 내림차순 정렬 / Counter 사용시 이미 내림차순 정렬됨.\n",
        "  vocab_sorted=sorted(cnt.items(), key = lambda x:x[1], reverse = True)\n",
        "  #print(vocab_sorted)\n",
        "\n",
        "# 2개 이상만 사전에 포함시키기.\n",
        "  for idx,(word,freq) in enumerate(vocab_sorted):\n",
        "      if freq<1:\n",
        "          continue\n",
        "      word_to_index[word]=idx+1\n",
        "      index_to_word[idx+1]=word\n",
        "    \n",
        "# 없는 단어 인덱스 추가\n",
        "#word_to_index['OOV'] = len(word_to_index) + 1\n",
        "#index_to_word[len(index_to_word)+1] = 'OOV'\n",
        "\n",
        "word_to_index, index_to_word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "ye6SAE5G9qB8",
        "outputId": "760ae1b3-40a2-4f50-d48b-16a1eff8aaf2"
      },
      "source": [
        "encoded = []\n",
        "for s in corpus:\n",
        "    temp = []\n",
        "    for w in s:\n",
        "      temp.append(ft_word_to_index[w])\n",
        "    encoded.append(temp)\n",
        "print(encoded)"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-128-e8ce04574f4a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m       \u001b[0mtemp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mft_word_to_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mencoded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'ㅇㅝ'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sKxZxbXllNW"
      },
      "source": [
        "ft_word_to_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwDbeRZlkrgQ"
      },
      "source": [
        "#임베딩 벡터 만들기\n",
        "import numpy as np\n",
        "key = list(word_to_index.keys())\n",
        "fasttext = []\n",
        "for k in key:\n",
        "    fasttext.append(model.wv[k])\n",
        "\n",
        "fasttext=np.array(fasttext)"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7vQZCqKA79rj",
        "outputId": "7c248a1e-fdc5-4d4a-e57f-f5cc2fa6e806"
      },
      "source": [
        "fasttext.shape"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7819, 50)"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXmqu9AjzAcz"
      },
      "source": [
        "... 자소분리를 하지 않은 토큰을 fasttext에 적용하는 방법"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_p2sD468zG-R",
        "outputId": "c6b502eb-80f6-4941-eb03-eaa2a1895ec0"
      },
      "source": [
        "#fasttext 적용하기  \n",
        "from gensim.models import FastText\n",
        "model_no = FastText(df['token_1'], vector_size=10, workers=4, sg=1, word_ngrams=1, min_count =3) #iter=2,\n",
        "print(f\"학습 소요 시간 : {model_no.total_train_time}\")"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "학습 소요 시간 : 3.216515234000326\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ToA9YhwizKbJ"
      },
      "source": [
        "#fasttext vocab만들기\n",
        "model_no.build_vocab(df['token_1'])"
      ],
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xTmIo1aJzOYl",
        "outputId": "982c4768-34b2-4dd8-aadf-c3a9bdf872f8"
      },
      "source": [
        "#fasttext 훈련시키기\n",
        "model_no.train(\n",
        "    df['token_1'], epochs=model.epochs,\n",
        "    total_examples=model.corpus_count, total_words=model.corpus_total_words,\n",
        ")"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(621830, 852325)"
            ]
          },
          "metadata": {},
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xORpQEVCzSJt"
      },
      "source": [
        "ft_word_to_index_no = model_no.wv.key_to_index\n",
        "ft_index_to_word_no = dict([(value, key) for (key, value) in ft_word_to_index_no.items()])"
      ],
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "-ilhA3jvzX-m",
        "outputId": "2a511be2-1d2b-4284-b867-e43abcaaa1cd"
      },
      "source": [
        "encoded_no = []\n",
        "for s in df['token_1']:\n",
        "    temp = []\n",
        "    for w in s:\n",
        "      temp.append(ft_word_to_index_no[w])\n",
        "    encoded_no.append(temp)\n",
        "print(encoded_no)"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-130-795681283674>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m       \u001b[0mtemp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mft_word_to_index_no\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mencoded_no\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_no\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: '워'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPLm_eER59RT"
      },
      "source": [
        "##- 피상담자의 문장을보고 상담해주는 글을 만들어보자 (seqGAN)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3YBdZcqdf5d"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test = train_test_split(df['counseling'], test_size= 0.2, random_state=1234)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PW3u3b7A33bb"
      },
      "source": [
        "def decode_review(text):\n",
        "    \"\"\"\n",
        "    word_index를 받아 text를 sequence 형태로 반환하는 함수입니다.\n",
        "    \"\"\"\n",
        "    return ' '.join([ft_index_to_word.get(i,'?') for i in text])"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "id": "ViYQj89V38MW",
        "outputId": "bc75064d-83e0-4741-dc1f-8d84f7c0dff1"
      },
      "source": [
        "join_jamos(decode_review(encoded[0]))"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-127-16e3c70a6d5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mjoin_jamos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecode_review\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CeHU_8bL4BY4"
      },
      "source": [
        "sentences = [join_jamos(decode_review(idx)) for idx in encoded]"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "qPEYmjbGWcGc",
        "outputId": "be5079f7-0072-4dba-f94c-24cc85d559e6"
      },
      "source": [
        "df['counseling'][4]"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'상담 자체로도 큰 효과를 볼 수 있는 경우도 많기에 학생이시라면 학교사회복지사를 찾아가는 것을 추천드려요 그분과 상담을 진행하면서 큰 도움이 될 수도 있을 것이며 이분을 통해 지역사회에 있는 상담이 가능한 곳에 연계하여 진행 할 수 있답니다. 학교에서 상담을 진행하는 경우 비용이 발생하지 않지만 지역사회에 있는 센터를 이용할 경우 무료로 이용할 수도 혹은 비용이 발생할 수 있기에 이것에 대하여 고려해야할 필요가 있을것 같아요. 질문에 대한 답변이 되었다면 채택부탁드립니다.'"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "wTw3AUGOUVM1",
        "outputId": "957a8426-3ed2-4e76-a61a-b9ee47acb438"
      },
      "source": [
        "sentences[4]"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'상담 감정 도 ㄴ 소년 ㄹ 는 도 번호 친구 시 드리 ㅁ ㄴ가 사이버 ㄴ 5 는 집중력 전문 선생님 올리 상담 8 플러스 ㄴ 도움 ㄹ 도 고민 안녕 글로 는 상담 가능 ㄴ 무겁 요즘 8 ㄹ 친구 기 상담 8 는 요즘 발생 글로 는 센터 이용 ㄹ 무료 이용 ㄹ 으니 요즘 발생 ㄹ 번호 시 어야 ㄹ 읽 아요 자주 ㄴ 답변 친구 무료 청소년 안녕 ㅂ니다'"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        },
        "id": "m73x6ZNudyNK",
        "outputId": "8e4ede21-2198-4e08-9a58-986b466e050b"
      },
      "source": [
        "import numpy as np\n",
        "#import tensorflow as tf\n",
        "import random\n",
        "from dataloader import Gen_Data_loader, Dis_dataloader\n",
        "#from generator_1 import Generator\n",
        "#from discriminator import Discriminator\n",
        "from rollout import ROLLOUT\n",
        "import pickle\n",
        "import time\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#########################################################################################\n",
        "#  Generator  Hyper-parameters\n",
        "######################################################################################\n",
        "EMB_DIM = 30 # embedding dimension (pretrained: 200, pk: 30)\n",
        "HIDDEN_DIM = 300 # hidden state dimension of lstm cell\n",
        "SEQ_LENGTH = 30 # sequence length\n",
        "START_TOKEN = 0\n",
        "PRE_EPOCH_NUM = 120  # supervise (maximum likelihood estimation) epochs\n",
        "SEED = 88\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "#########################################################################################\n",
        "#  Discriminator  Hyper-parameters\n",
        "#########################################################################################\n",
        "dis_embedding_dim = EMB_DIM\n",
        "dis_filter_sizes = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 30]\n",
        "dis_num_filters = [100, 200, 200, 200, 200, 100, 100, 100, 100, 100, 160, 160]\n",
        "dis_dropout_keep_prob = 0.75\n",
        "dis_l2_reg_lambda = 0.2\n",
        "dis_batch_size = 64\n",
        "\n",
        "#########################################################################################\n",
        "#  Basic Training Parameters\n",
        "#########################################################################################\n",
        "TOTAL_BATCH = 200\n",
        "generated_num = 100\n",
        "sample_num = 10\n",
        "\n",
        "# original seqgan parameter\n",
        "# HIDDEN_DIM = 32\n",
        "# PRE_EPOCH_NUM = 120\n",
        "# TOTAL_BATCH = 200\n",
        "# generated_num = 10000\n",
        "\n",
        "positive_file = df['counseling']\n",
        "negative_file = 'save/negative_sample.txt'\n",
        "eval_file = 'save/eval_file.txt'\n",
        "\n",
        "real_data = encoded\n",
        "\n",
        "vocab_to_int = word_to_index\n",
        "\n",
        "int_to_vocab = index_to_word\n",
        "print(int_to_vocab)\n",
        "\n",
        "word_embedding_matrix = fasttext\n",
        "word_embedding_matrix = fasttext.astype(np.float32)\n",
        "\n",
        "#real_data_vocab = [[int_to_vocab[i] for i in sample if int_to_vocab[i] != '<PAD>'] for sample in real_data]\n",
        "real_data_vocab = [decode_review(encoded[i]) for i in range(0,len(encoded))]\n",
        "real_data_vocab = join_jamos(real_data_vocab)\n",
        "print(len(real_data_vocab))\n",
        "\n",
        "\n",
        "def generate_samples(sess, trainable_model, batch_size, generated_num, output_file, word_embedding_matrix):\n",
        "    # Generate Samples\n",
        "    generated_samples = []\n",
        "    for _ in range(int(generated_num / batch_size)):\n",
        "        generated_samples.extend(trainable_model.generate(sess, word_embedding_matrix))\n",
        "\n",
        "    with open(output_file, 'w') as fout:\n",
        "        for poem in generated_samples:\n",
        "            buffer = ' '.join([str(x) for x in poem]) + '\\n'\n",
        "            fout.write(buffer)\n",
        "\n",
        "\n",
        "def pre_train_epoch(sess, trainable_model, data_loader, word_embedding_matrix):\n",
        "    # Pre-train the generator using MLE for one epoch\n",
        "    supervised_g_losses = []\n",
        "    data_loader.reset_pointer()\n",
        "\n",
        "    for it in range(data_loader.num_batch):\n",
        "        batch = data_loader.next_batch()\n",
        "        _, g_loss = trainable_model.pretrain_step(sess, batch, word_embedding_matrix)\n",
        "        supervised_g_losses.append(g_loss)\n",
        "\n",
        "    return np.mean(supervised_g_losses)\n",
        "\n",
        "\n",
        "def make_sample(eval_file, int_to_vocab, sample_num):\n",
        "    samples = []\n",
        "    with open(eval_file, 'r') as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            line = line.split()\n",
        "            parse_line = [int(x) for x in line]\n",
        "            samples.append(parse_line)\n",
        "\n",
        "    sample_int = samples[:sample_num]\n",
        "    sample_vocab = [[int_to_vocab[i] for i in sample] for sample in sample_int]\n",
        "    sample_vocab = [' '.join(sample) for sample in sample_vocab]\n",
        "\n",
        "    return sample_vocab\n",
        "\n",
        "################################## main() #########################################\n",
        "\n",
        "# 시간측정\n",
        "start_time = time.time()\n",
        "\n",
        "tf.compat.v1.reset_default_graph()\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "gen_data_loader = Gen_Data_loader(BATCH_SIZE, SEQ_LENGTH)\n",
        "vocab_size = len(vocab_to_int)  # 6447\n",
        "print(vocab_size)\n",
        "dis_data_loader = Dis_dataloader(BATCH_SIZE, SEQ_LENGTH)\n",
        "\n",
        "generator = Generator(vocab_size, BATCH_SIZE, EMB_DIM, HIDDEN_DIM, SEQ_LENGTH, START_TOKEN)\n",
        "discriminator = Discriminator(sequence_length=SEQ_LENGTH, num_classes=2, word_embedding_matrix=word_embedding_matrix,\n",
        "                              embedding_size=dis_embedding_dim, filter_sizes=dis_filter_sizes,\n",
        "                              num_filters=dis_num_filters, l2_reg_lambda=dis_l2_reg_lambda)\n",
        "\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "\n",
        "sess = tf.Session(config=config)\n",
        "saver = tf.train.Saver()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "# First, use the oracle model to provide the positive examples, which are sampled from the oracle data distribution\n",
        "#  pre-train generator\n",
        "#gen_data_loader.create_batches(positive_file)\n",
        "#gen_sample = open('save/pretrain_sample.txt', 'w')\n",
        "#print('Start pre-training...')\n",
        "#gen_sample.write('pre-training...\\n')\n",
        "for epoch in range(PRE_EPOCH_NUM):\n",
        "    loss = pre_train_epoch(sess, generator, gen_data_loader, word_embedding_matrix)\n",
        "    if epoch % 5 == 0:\n",
        "        generate_samples(sess, generator, BATCH_SIZE, generated_num, eval_file, word_embedding_matrix)\n",
        "        sample_vocab = make_sample(eval_file, int_to_vocab, sample_num)\n",
        "\n",
        "        print('pre-train epoch ', epoch)\n",
        "\n",
        "        buffer = 'epoch:\\t' + str(epoch) + '\\n'\n",
        "        gen_sample.write(buffer)\n",
        "        for sample in sample_vocab:\n",
        "            print(sample)\n",
        "            buffer = sample + '\\n'\n",
        "            gen_sample.write(buffer)\n",
        "\n",
        "#  pre-train discriminator\n",
        "print('Start pre-training discriminator...')\n",
        "# Train 3 epoch on the generated data and do this for 50 times\n",
        "for _ in range(25):\n",
        "    generate_samples(sess, generator, BATCH_SIZE, generated_num, negative_file, word_embedding_matrix)\n",
        "    dis_data_loader.load_train_data(positive_file, negative_file)\n",
        "    for _ in range(3):\n",
        "        dis_data_loader.reset_pointer()\n",
        "        for it in range(dis_data_loader.num_batch):\n",
        "            x_batch, y_batch = dis_data_loader.next_batch()\n",
        "            feed = {\n",
        "                discriminator.input_x: x_batch,\n",
        "                discriminator.input_y: y_batch,\n",
        "                discriminator.dropout_keep_prob: dis_dropout_keep_prob\n",
        "            }\n",
        "            _ = sess.run(discriminator.train_op, feed)\n",
        "\n",
        "rollout = ROLLOUT(generator, 0.8, word_embedding_matrix)\n",
        "\n",
        "print('#########################################################################')\n",
        "print('Start Adversarial Training...')\n",
        "gen_sample.write('adversarial training...\\n')\n",
        "for total_batch in range(TOTAL_BATCH):\n",
        "    # Train the generator for one step\n",
        "    for it in range(1):\n",
        "        samples = generator.generate(sess, word_embedding_matrix)\n",
        "        rewards = rollout.get_reward(sess, samples, 16, discriminator)\n",
        "        feed = {generator.x: samples, generator.rewards: rewards, generator.word_embedding_matrix: word_embedding_matrix}\n",
        "        _ = sess.run(generator.g_updates, feed_dict=feed)\n",
        "\n",
        "    # Test\n",
        "    if total_batch % 5 == 0 or total_batch == TOTAL_BATCH - 1:\n",
        "        generate_samples(sess, generator, BATCH_SIZE, generated_num, eval_file, word_embedding_matrix)\n",
        "        sample_vocab = make_sample(eval_file, int_to_vocab, sample_num)\n",
        "\n",
        "        print('total_batch: ', total_batch)\n",
        "\n",
        "        buffer = 'epoch:\\t' + str(total_batch) + '\\n'\n",
        "        gen_sample.write(buffer)\n",
        "        for sample in sample_vocab:\n",
        "            print(sample)\n",
        "            buffer = sample + '\\n'\n",
        "            gen_sample.write(buffer)\n",
        "\n",
        "    # Update roll-out parameters\n",
        "    rollout.update_params()\n",
        "\n",
        "    # Train the discriminator\n",
        "    for _ in range(5):\n",
        "        generate_samples(sess, generator, BATCH_SIZE, generated_num, negative_file, word_embedding_matrix)\n",
        "        dis_data_loader.load_train_data(positive_file, negative_file)\n",
        "\n",
        "        for _ in range(3):\n",
        "            dis_data_loader.reset_pointer()\n",
        "            for it in range(dis_data_loader.num_batch):\n",
        "                x_batch, y_batch = dis_data_loader.next_batch()\n",
        "                feed = {\n",
        "                    discriminator.input_x: x_batch,\n",
        "                    discriminator.input_y: y_batch,\n",
        "                    discriminator.dropout_keep_prob: dis_dropout_keep_prob\n",
        "                }\n",
        "                _ = sess.run(discriminator.train_op, feed)\n",
        "\n",
        "    if total_batch % 5 == 0 or total_batch == TOTAL_BATCH - 1:\n",
        "        saver.save(sess, './checkpoint/seqGAN_ours')\n",
        "\n",
        "gen_sample.close()\n",
        "\n",
        "# 걸린 시간 출력\n",
        "time_check = \"--- total {} seconds ---\".\\\n",
        "    format(time.time() - start_time)\n",
        "print(time_check)\n",
        "\n",
        "generate_samples(sess, generator, BATCH_SIZE, generated_num, eval_file, word_embedding_matrix)\n",
        "\n",
        "samples = make_sample(eval_file, int_to_vocab, generated_num)\n",
        "samples = [[word for word in sample.split() if word != 'UNK'] for sample in samples]\n",
        "samples = [' '.join(sample) for sample in samples]\n",
        "\n",
        "#f = open('./save/final_output_vocab.txt', 'w')\n",
        "#for token in samples:\n",
        "#    token = token + '\\n'\n",
        "#    f.write(token)\n",
        "#f.close()\n",
        "\n",
        "# write the training time\n",
        "#f = open('./save/_parameters.txt', 'w')\n",
        "#f.write(\"Training time : {}\\n\".format(time_check))\n",
        "#f.write(\"add <start> signal as zero in word2vec lookup table\\n\")\n",
        "#f.close()"
      ],
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{1: 'ㅅㅏㅇㄷㅏㅁ', 2: 'ㄱㅗ', 3: 'ㅅㅣ', 4: 'ㅇㅡㄴ', 5: 'ㅁㅜㄴㅈㅏ', 6: 'ㅅㅔㄴㅌㅓ', 7: 'ㅇㅓㅇㅛ', 8: 'ㄴ', 9: 'ㅌㅗㄱ', 10: 'ㄱㅏㄴㅡㅇ', 11: 'ㅂㅗㄱㅈㅣ', 12: 'ㅅㅏㅇㅣㅂㅓ', 13: 'ㅅㅏㅇㄷㅏㅁㅇㅝㄴ', 14: 'ㄴㅐㅇㅛㅇ', 15: 'ㄱㅔㅆ', 16: 'ㅇㅓㅅㅓ', 17: 'ㅇㅣㅇㅑㄱㅣ', 18: 'ㄴㅡㄴ', 19: 'ㅋㅏ', 20: 'ㄹ', 21: 'ㄹㄱㅔㅇㅛ', 22: '1388', 23: 'ㅂㅗㄴㅐ', 24: 'ㅊㅣㄴㄱㅜ', 25: 'ㄱㅣ', 26: 'ㄷㅏㅂㅂㅕㄴ', 27: '24', 28: 'ㅇㅣㅇㅛㅇ', 29: 'ㅇㅏㄴㄴㅕㅇ', 30: 'ㅎㅏㅅㅔ', 31: 'ㅇㅛ', 32: 'ㅅㅓㅇㅜㄹㅅㅣ', 33: 'ㅊㅓㅇㅅㅗㄴㅕㄴ', 34: 'ㅂㄴㅣㄷㅏ', 35: 'ㄴㅣ', 36: 'ㅁ', 37: 'ㅇㅗㄹㄹㅣ', 38: 'ㅈㅜㅅㅣㄴ', 39: 'ㄱㅡㄹ', 40: 'ㅇㅣㄺ', 41: 'ㅇㅏㅆ', 42: 'ㅈㅏㅅㅔ', 43: 'ㅇㅛㅈㅡㅁ', 44: 'ㅇㅜㅇㅜㄹ', 45: 'ㄱㅏㅁㅈㅓㅇ', 46: 'ㅈㅏㅈㅜ', 47: 'ㄴㅡㄲㅣ', 48: 'ㅁㅏㅇㅡㅁ', 49: 'ㅁㅜㄱㅓㅂ', 50: 'ㅎㅣㅁㄷㅡㄹ', 51: 'ㅇㅡㄴㄷㅔ', 52: 'ㅂㅗㄷㅏ', 53: 'ㄱㅡㄹㄹㅗ', 54: 'ㅍㅕㄴㅎㅏ', 55: 'ㄴㄱㅏ', 56: 'ㅇㅏㅇㅛ', 57: 'ㅇㅣㄴㅏ', 58: 'ㄱㅣㄱㅘㄴ', 59: 'ㅇㅏㄴㄴㅐ', 60: 'ㄷㅡㄹㅣ', 61: 'ㅎㅐㄴㄷㅡㅍㅗㄴ', 62: 'ㅂㅗㄴㅐㄱㅣ', 63: 'ㅅㅜㅅㅣㄴㅈㅏ', 64: 'ㅂㅓㄴㅎㅗ', 65: 'ㅆㅡ', 66: 'ㄱㅗㅁㅣㄴ', 67: 'ㄱㅓㄴㅏ', 68: 'ㅋㅏㅋㅏㅇㅗ', 69: 'ㅍㅡㄹㄹㅓㅅㅡ', 70: 'ㅁㅐㅈ', 71: 'ㅋㅏㅋㅏㅇㅗㅌㅗㄱㅇㅡ', 72: 'ㅁㅕㄴ', 73: 'ㅈㅓㄴㅁㅜㄴ', 74: 'ㅅㅓㄴㅅㅐㅇㄴㅣㅁ', 75: 'ㅁㅜㄹㅛ', 76: 'ㄷㅔㅇㅣㅌㅓ', 77: 'ㅇㅛㄱㅡㅁ', 78: 'ㅇㅣㅇㅛㅇㅈㅏ', 79: 'ㅂㅜㄷㅏㅁ', 80: 'ㅇㅔㅇㅛ', 81: 'ㄴㅣㅁ', 82: 'ㄷㅡㄹㅇㅓㅂㅗ', 83: 'ㄴㅔㅇㅛ', 84: 'ㅎㅗㅁㅍㅔㅇㅣㅈㅣ', 85: 'ㅇㅗㅅㅣ', 86: 'ㄱㅔㅅㅣㅍㅏㄴ', 87: 'ㅊㅐㅌㅣㅇ', 88: 'ㅁㅗㄱ', 89: '12', 90: '7', 91: '5', 92: '6', 93: 'ㅈㅔㅇㅚ', 94: 'ㄷㅗ', 95: 'ㅇㅡㄴㅣ', 96: 'ㅅㅔㅇㅛ', 97: 'ㅇㅣㄴㅏㅁㅏ', 98: 'ㄷㅗㅇㅜㅁ', 99: 'ㅂㅏㄹㅏ', 100: 'ㅅㅓㅇㅜㄹ', 101: 'ㅅㅣㅊㅓㅇ', 102: 'ㅅㅗㄴㅕㄴ', 103: 'ㄷㅡㄹㅣㅁ', 104: '8', 105: 'ㅈㅣㅂㅈㅜㅇㄹㅕㄱ', 106: 'ㅇㅜㅇㅠㅂㅜㄷㅏㄴ', 107: '9', 108: 'ㅂㅏㄴㅂㅗㄱ', 109: 'ㅇㅟ', 110: 'ㅇㅓㄴㄱㅡㅂ', 111: 'ㅈㅡㅇㅅㅏㅇ', 112: 'ㅅㅐㅇㅎㅘㄹ', 113: 'ㅅㅣㅁㄱㅏㄱ', 114: 'ㅇㅠㅂㅏㄹ', 115: 'ㄴㅐㄹㅣ', 116: 'ㅈㅏㅅㅔ', 117: 'ㄱㅡㄹ', 118: 'ㄷㅏㄹㅜ', 119: 'ㅇㅡㄴㅣ', 120: 'ㄱㅣ', 121: 'ㅂㅏㄹㅏ', 122: '222049504666', 123: 'ㄱㅘㄴㄹㅣㅂㅓㅂ', 124: 'ㅌㅔㅅㅡㅌㅡ', 125: '...', 126: 'ㄱㅕㅇㅜㄹ', 127: 'ㅊㅗㅇ', 128: 'ㅇㅓㄷㅗ', 129: 'ㅈㅣㅇㅕㄱㅂㅕㄹ', 130: 'ㅇㅜㄴㅇㅕㅇ', 131: 'ㅅㅏㅇ', 132: 'ㅊㅏㅇㅣ', 133: 'ㅊㅏㅈㅇㅏㅂㅗ', 134: 'ㅇㅡㅁㅡㄹㅗ', 135: 'ㄱㅓㅈㅜ', 136: 'ㅈㅣㄱㅈㅓㅂ', 137: 'ㅇㅓㅅㅓ', 138: 'ㅎㅘㄱㅇㅣㄴ', 139: 'ㅇㅓㅇㅑ', 140: 'ㅋㅗㄹ', 141: 'ㅂㅓㄴㅎㅗ', 142: '1396', 143: 'ㅇㅏㄹㅇㅏㅂㅗ', 144: 'ㅇㅣㄴㅏㅁㅏ', 145: 'ㄱㅣ', 146: 'ㅂㅏㄹㅏ', 147: 'ㄹㄱㅔㅇㅛ', 148: 'ㅁㅏ', 149: '9', 150: '24', 151: 'ㅇㅣㅇㅛㅇ', 152: 'ㅅㅣ', 153: 'ㅈㅜㅈㅔ', 154: 'ㄷㅡㄴ', 155: 'ㅊㅏㅈ', 156: 'ㄷㅡㄹㅣㅁ', 157: 'ㄷㅏㅁㄷㅏㅇㅈㅏ', 158: 'ㄷㅐㅍㅛ', 159: 'ㅇㅑㅇㅎㅐ', 160: 'ㄱㅏㄱㅅㅓ', 161: 'ㅅㅓㅁㅕㅇ', 162: 'ㄷㅟ', 163: 'ㅅㅡㅋㅔㅈㅜㄹ', 164: 'ㄱㅖㅎㅚㄱ', 165: 'ㅍㅏㅌㅡㄴㅓㅅㅣㅂ', 166: 'ㅎㅛㄹㅕㄱ', 167: 'ㅂㅏㄹㅅㅐㅇ', 168: 'ㄱㅔ', 169: 'ㅊㅏㅁㅇㅕ', 170: 'ㅁㅜㄴㅇㅢ', 171: 'ㅅㅏㅎㅏㅇ', 172: 'ㅇㅡㅅㅣ', 173: 'ㅇㅏㄹㅐ', 174: 'ㅇㅕㄴㄹㅏㄱ', 175: 'ㅅㅣㄱㅣ', 176: 'ㄱㅏㅁㅅㅏ', 177: 'ㅇㅣㅁㅔㅇㅣㄹ', 178: '2325', 179: '2326', 180: 'ㅍㅔㅇㅣㅅㅡ', 181: 'ㅂㅜㄱ', 182: 'ㅍㅔㅇㅣㅈㅣ', 183: 'ㅈㅗㅎㅇㅏㅇㅛ', 184: 'ㄴㅜㄹㅡ', 185: 'ㄴㅠㅅㅡ', 186: 'ㅇㅓㅂㄷㅔㅇㅣㅌㅡ', 187: 'ㅂㅗㅅㅣ', 188: 'ㅅㅡㅌㅏ', 189: 'ㄱㅡㄹㅐㅁ', 190: 'ㅋㅏㅋㅏㅇㅗ', 191: 'ㅌㅗㄱ', 192: 'ㅁ', 193: 'ㅁㅜ', 194: 'ㅎㅗㅎㅡㅂㅈㅡㅇ', 195: 'ㅇㅑㅌ', 196: 'ㅁㅏㄱ', 197: 'ㅂㅣㅇㅕㅁ', 198: 'ㅇㅣㅁㅕㅇ', 199: 'ㄴㅏㄴㅊㅓㅇ', 200: 'ㅎㅘㄴㅊㅓㅇ', 201: 'ㅌㅗㅇ', 202: 'ㅇㅣㅂ', 203: 'ㅁㅏㄹㅡㅁ', 204: 'ㄱㅜㄱㅏㅇ', 205: 'ㅅㅓㄴㅈㅏㅁ', 206: 'ㄴㅡㅈㅈㅏㅁ', 207: 'ㅇㅑㄴㅛㅈㅡㅇ', 208: 'ㅈㅏㅁㄲㅗㄷㅐ', 209: 'ㄹㅐㅁ', 210: 'ㅁㅗㅇㅇㅠㅂㅕㅇ', 211: 'ㅅㅏㅈㅣ', 212: 'ㅂㅏㄹㅈㅏㄱ', 213: 'ㅇㅜㄴㄷㅗㅇㅈㅡㅇ', 214: 'ㅁㅜㄱㅣㄹㅕㄱ', 215: 'ㅇㅑㄱㅕㅇㅈㅡㅇ', 216: 'ㄱㅣㅁㅕㄴㅈㅡㅇ', 217: 'ㅈㅜㄱㅏㄴ', 218: 'ㅈㅗㄹ', 219: 'ㄹㅣㅁ', 220: 'ㅊㅜㄴㄱㅗㄴㅈㅡㅇ', 221: 'ㄸㅗㄴㅡㄴ', 222: 'ㄷㅏㅇㅑㅇ', 223: 'ㄱㅣㅇㅓㄱㄹㅕㄱ', 224: 'ㅈㅣㅂㅈㅜㅇㄹㅕㄱ', 225: 'ㅅㅏㄱㅗㄹㅕㄱ', 226: 'ㅎㅏㄱㅅㅡㅂ', 227: 'ㄴㅡㅇㄹㅕㄱ', 228: 'ㅈㅓㅎㅏ', 229: 'ㅅㅣㄴㄱㅕㅇㄱㅘ', 230: 'ㅁㅣㄴ', 231: 'ㅅㅣㄴㄱㅕㅇ', 232: 'ㅅㅚㅇㅑㄱ', 233: 'ㅇㅖㅁㅣㄴ', 234: 'ㅅㅓㅇㄱㅕㄱ', 235: 'ㅈㅓㅇㅅㅓ', 236: 'ㄱㅗㅇㅎㅘㅇ', 237: 'ㅈㅏㅎㅐ', 238: 'ㅇㅜㅇㅜㄹㅈㅡㅇ', 239: 'ㅅㅗㅂㅕㄴ', 240: 'ㅂㅜㅈㅗㅇ', 241: 'ㄱㅘㄴㅈㅓㄹ', 242: 'ㄱㅡㄴㅇㅠㄱ', 243: 'ㄱㅕㅇㅈㅣㄱ', 244: 'ㅁㅏㄴㅅㅓㅇ', 245: 'ㅍㅣㄹㅗ', 246: 'ㅂㅜㅈㅓㅇㅁㅐㄱ', 247: 'ㅊㅓㄴㅅㅣㄱ', 248: 'ㄱㅏㅁㄱㅣ', 249: 'ㄴㅚ', 250: 'ㄱㅘㄴㄹㅕㄴ', 251: 'ㄷㅗㄹㅇㅕㄴㅅㅏ', 252: 'ㅅㅣㅁㄱㅏㄱ', 253: 'ㅇㅓㄹㅕㅂ', 254: 'ㅎㅕㄴㅅㅣㄹ', 255: 'ㅇㅣㄹㅅㅣㅈㅓㄱ', 256: 'ㅇㅘㄴㅎㅘ', 257: 'ㅇㅟㅇㅢ', 258: 'ㅎㅐㄷㅏㅇ', 259: 'ㅇㅏㄹㄹㅣ', 260: 'ㄱㅓㄹㅗㄴ', 261: 'ㄷㅡ', 262: 'ㅁㅐㅌㅡ', 263: 'ㅁㅐㅌㅡㄹㅣㅅㅡ', 264: 'ㅌㅡㄱㅎㅣ', 265: 'ㅇㅣㄴㅈㅜㄹ', 266: 'ㅅㅏㅇㅛㅇ', 267: 'ㄱㅏㄴㅡㅇ', 268: 'ㅂㅓㅂ', 269: 'ㄹㅕ', 270: '...', 271: 'ㅋㅡㄹㄹㅣㄱ', 272: 'ㅂㅜㄴ', 273: 'ㅇㅣㄴㅐ', 274: 'ㅈㅏㅁㄷㅡㄹ', 275: 'ㅈㅔㄷㅐ', 276: 'ㅅㅜㅁㅕㄴㅈㅔ', 277: 'ㅇㅠㄷㅗ', 278: 'ㅂㅗㄱㅇㅛㅇ', 279: 'ㅇㅣㄴㅅㅐㅇ', 280: 'ㄱㅏㅊㅣ', 281: 'ㅂㅏㄲㅜ', 282: 'ㅂㅓㄹㅣ', 283: 'ㅎㅐㅇㅂㅗㄱ', 284: 'ㅂㅐㄱ', 285: 'ㅇㅖㅂㅏㅇ', 286: 'ㄱㅗㅊㅣ', 287: 'ㅂㅕㅇㅇㅣ', 288: 'ㅂㅜㅈㅏㄱㅇㅛㅇ', 289: 'ㅅㅓㅇㅈㅏㅇ', 290: 'ㅇㅣㄹㅇㅡㅋㅣ', 291: 'ㅂㅏㄹㅅㅐㅇ', 292: 'ㅈㅐㅅㅐㅇ', 293: 'ㅈㅜㄹㅇㅣ', 294: 'ㅁㅕ', 295: 'ㅅㅓㅇㅈㅏㅇㄷㅗ', 296: 'ㅊㅗㄱㅈㅣㄴ', 297: 'ㅁㅕㅇㅇㅢ', 298: 'ㅇㅢㅅㅏ', 299: 'ㅇㅓㄷㅗ', 300: 'ㄷㅓㄹ', 301: 'ㅂㅗㄱㅎㅏㅂ', 302: 'ㅈㅣㄴㅣ', 303: 'ㅎㅘㄴㅈㅏ', 304: 'ㅈㅜㅇㄷㅏㄴ', 305: 'ㅊㅓㅅㄴㅏㄹ', 306: 'ㅅㅣㅈㅏㄱ', 307: 'ㅇㅘㄴㅊㅣ', 308: 'ㅅㅜㅈㅜㄴ', 309: 'ㄱㅣㄹ', 310: 'ㅅㅔㅇㅝㄹ', 311: 'ㅇㅠㅈㅣ', 312: 'ㅅㅏㅇㅅㅣㄱ', 313: 'ㄴㅓㅁ', 314: 'ㅁㅏ', 315: 'ㅁㅏㅎㅏㄴ', 316: 'ㅇㅢㅅㅣㅁ', 317: 'ㅅㅣㄴㄱㅗ', 318: 'ㅇㅢㅎㅏ', 319: 'ㄱㅕㅇㅊㅏㄹ', 320: 'ㄱㅓㅁㅊㅏㄹ', 321: 'ㄱㅗㅇㅈㅓㅇ', 322: 'ㄱㅓㄹㅐ', 323: 'ㅇㅟㅇㅝㄴㅎㅚ', 324: 'ㅅㅣㄱ', 325: 'ㅊㅓ', 326: 'ㅅㅜㅊㅏㄹㅖ', 327: 'ㅆㅣㄱ', 328: 'ㅈㅗㅅㅏ', 329: 'ㅊㅔㅎㅓㅁㅈㅏ', 330: 'ㅈㅣㄱㅈㅓㅂ', 331: 'ㅈㅏㄱㅅㅓㅇ', 332: 'ㅎㅘㄱㅇㅣㄴ', 333: 'ㄱㅓㅁㅈㅡㅇ', 334: 'ㅇㅏㅇㅣㅍㅣ', 335: 'ㄴㅗㅊㅜㄹ', 336: 'ㅊㅓㄴ', 337: 'ㄱㅓㄴㅇㅢ', 338: 'ㄱㅗㅅ', 339: 'ㅁㅗㄱㅍㅛ', 340: 'ㅍㅕㄴㅎㅏ', 341: 'ㄲㅏ', 342: 'ㄷㅗㄱㅇㅑㄱ', 343: '?', 344: '8', 345: '0', 346: '81', 347: '91', 348: '95', 349: '84', 350: '94', 351: '82', 352: '1', 353: 'ㅇㅟ', 354: 'ㅈㅔㅍㅜㅁ', 355: 'ㅅㅓㄴㅌㅐㄱ', 356: 'ㅍㅕㅇㅅㅐㅇ', 357: 'ㄱㅖㅅㅗㄱ', 358: 'ㅇㅢㅌㅏㄱ', 359: 'ㄷㅗㄹㅣㅇㅓ', 360: 'ㄴㅏㄴㅊㅣㅂㅕㅇ', 361: 'ㅂㅜㄹㅊㅣㅂㅕㅇ', 362: 'ㅇㅛ', 363: 'ㅌㅡㄱㅊㅐ', 364: 'ㄱㅘㄴ', 365: 'ㅂㅓㅁㅇㅣㄴ', 366: 'ㄱㅡㄹㅣ', 367: 'ㅂㅗㅅㅣㄱㅣ', 368: 'ㅎㅘㄹㄷㅗㅇ', 369: 'ㄱㅠㅊㅣㄱ', 370: 'ㅂㅓㅁㅈㅚㅎㅏㄱ', 371: 'ㅇㅜㄹㅣㄴㅏㄹㅏ', 372: 'ㄱㅜㄱㄹㅣㅂ', 373: 'ㄱㅜㄱㄱㅘㅅㅜ', 374: 'ㄷㅐ', 375: 'ㄱㅓㅁㅊㅏㄹㅊㅓㅇ', 376: 'ㄱㅜㄱㅂㅏㅇㅂㅜ', 377: 'ㅁㅕㅇㅊㅣㅇ', 378: 'ㅇㅣㄹㅂㅏㄴㅈㅓㄱ', 379: 'ㅇㅣㄴㅌㅓㄴㅔㅅ', 380: 'ㅌㅏㄹㄹㅏㄱ', 381: '101', 382: '120', 383: 'ㅎㅏㅇㄱㅗㅇ', 384: 'ㅈㅗㅇ', 385: 'ㅎㅏㄱㄷㅐ', 386: 'ㅍㅗㄱㅂㅏㄹ', 387: 'ㅁㅜㄹ', 388: '43', 389: 'ㅅㅣㄹㅇㅛㅇ', 390: 'ㅌㅔㄴㅓ', 391: 'ㅂㅔㅌㅡㄴㅏㅁ', 392: 'ㅌㅐㄱㅜㄱ', 393: 'ㄹㅓㅅㅣㅇㅏ', 394: 'ㅇㅏㄴㅂㅗ', 395: '15', 396: 'ㅇㅠㄷㅗ', 397: 'ㄱㅓㅁㄷㅗ', 398: 'ㄱㅕㄱ', 399: 'ㅅㅏㄱㅗ', 400: 'ㄱㅓㄴㅊㅜㄱ', 401: 'ㅌㅗㅁㅗㄱ', 402: 'ㅈㅓㄴㄱㅣ', 403: '30', 404: 'ㅅㅣㄴㅇㅣㅁ', 405: 'ㅅㅏㅇ', 406: 'ㅋㅗㄹㅗㄴㅏ', 407: 'ㄱㅏㅇㅇㅢ', 408: 'ㅎㅚ', 409: 'ㄱㅓㄹㅊㅣ', 410: 'ㅅㅐㅇㅎㅘㄹ', 411: 'ㅈㅔㅊㅜㄹ', 412: 'ㅁㅜㄴㅅㅣㄴ', 413: 'ㅇㅣㅁㅇㅛㅇㄹㅕㅇ', 414: 'ㅈㅏㄱㅇㅓㅂ', 415: 'ㅇㅡㅁㅕ', 416: 'ㄲㅏ', 417: 'ㅎㅕㅇㅅㅗㅂㅓㅂ', 418: 'ㅌㅐㄱ', 419: 'ㅎㅐㅇㅈㅓㅇㅂㅓㅂ', 420: 'ㄷㅐㅊㅔ', 421: 'ㅈㅓㅁㅅㅜ', 422: '550', 423: 'ㅅㅔㄹ', 424: 'ㅇㅡㅇㅅㅣㅈㅏ', 425: '22', 426: 'ㄷㅐㅂㅣ', 427: 'ㅎㅕㄱㅅㅣㄴ', 428: 'ㅈㅣㄴㅇㅟ', 429: 'ㅊㅟㄷㅡㄱ', 430: 'ㅇㅣㄴㅈㅐ', 431: 'ㅎㅑㅇㅎㅜ', 432: 'ㅈㅓㄹㅊㅏ', 433: '23', 434: 'ㄱㅕㅇㅅㅏ', 435: 'ㅈㅓㅇㅎㅐㅈㅣ', 436: 'ㅂㅗㄱㅁㅜ', 437: '2023', 438: 'ㄱㅐㅍㅕㄴㅇㅏㄴ', 439: 'ㅊㅜㄱㅏ', 440: 'ㅇㅗㅈㅓㄴ', 441: 'ㄷㅏㅂㅇㅏㄴㅈㅣ', 442: 'ㅁㅏ', 443: 'ㅋㅣㅇ', 444: 'ㅈㅓㄱㅇㅡㅇ', 445: 'ㅁㅕㄴㅅㅓ', 446: 'ㄷㅏㅁㄷㅏㅇㅈㅏ', 447: 'ㄲㅔ', 448: 'ㄱㅏㅅㅏㄴㅈㅓㅁ', 449: 'ㄱㅕㄹㄱㅕㄱ', 450: 'ㅇㅡㄴㅣ', 451: 'ㄱㅜㄱㅈㅓㄱ', 452: 'ㅂㅗㄱㅅㅜ', 453: 'ㅍㅣ', 454: 'ㅅㅓㅇㄴㅕㄴ', 455: 'ㅍㅏㅅㅏㄴ', 456: 'ㅂㅗㄱㄱㅝㄴ', 457: 'ㅎㅕㅇㅇㅢ', 458: 'ㅈㅐㅈㅣㄱ', 459: '355', 460: '356', 461: 'ㅈㅏㄹㅗ', 462: '300', 463: 'ㅁㅣㅅㅓㅇㄴㅕㄴ', 464: 'ㅁㅗㄱ', 465: 'ㅈㅓㅈㅣㄹㅡ', 466: 'ㅎㅗ', 467: 'ㄷㅐㅅㅏㅇ', 468: 'ㅅㅓㅇㅂㅓㅁㅈㅚ', 469: 'ㅇㅣㅁㅇㅛㅇㄱㅝㄴ', 470: 'ㄱㅝㄴ', 471: 'ㅁㅕㅇㅂㅜ', 472: 'ㅈㅣㄹㅂㅕㅇ', 473: 'ㅇㅝㄴㅎㅏ', 474: 'ㅅㅣㄴㅊㅓㅇ', 475: 'ㅈㅓㄴㅎㅘ', 476: 'ㅂㅏㄴㄷㅐ', 477: 'ㅇㅕㄹㅓ', 478: 'ㅊㅗㅇㅇㅣㄴㅇㅝㄴ', 479: 'ㅌㅡㄱㅂㅕㄹ', 480: 'ㅁㅏㅇㅡㅁ', 481: 'ㅇㅕㄴㄷㅗ', 482: 'ㅁㅣㅁㅏㄴ', 483: 'ㄱㅣㅎㅗㄴㅈㅏ', 484: 'ㅅㅏㅇㅎㅏㄴ', 485: 'ㄱㅜㄴㅇㅣㄴ', 486: 'ㅈㅣㅇㅝㄴ', 487: 'ㅊㅜㄹㅈㅔ', 488: 'ㅎㅕㅇㅌㅐ', 489: 'ㅊㅗㅇㅈㅓㅁ', 490: '200', 491: '1,000', 492: 'ㅎㅏㄱㄴㅕㄴ', 493: 'ㅊㅡㄱㅈㅓㅇ', 494: 'ㅇㅏㄱㄹㅕㄱ', 495: 'ㅇㅟㅅㅁㅗㅁ', 496: 'ㅇㅣㄹㅇㅡㅋㅣㄱㅣ', 497: 'ㅇㅓㅄㅇㅣ', 498: 'ㅊㅗㅇㄱㅕㅇ', 499: 'ㅅㅜㄴ', 500: 'ㅇㅑㅇㅅㅓㅇ', 501: 'ㅇㅕㅆ', 502: 'ㄱㅛㅅㅜㅈㅣㄴ', 503: 'ㅈㅓㄴㅁㅜㄴ', 504: 'ㅊㅚㄱㅗ', 505: 'ㅈㅣㄱㅡㅂ', 506: 'ㄹㅠ', 507: 'ㅈㅡㅇ', 508: 'ㅂㅗㅈㅣㄱ', 509: 'ㅈㅐㅈㅓㅇ', 510: 'ㄱㅏㅁㅅㅏ', 511: 'ㅅㅗㅇㅛ', 512: 'ㅇㅕㄴㅅㅜ', 513: 'ㄱㅡㄴㅁㅜㅈㅣ', 514: 'ㅅㅜㄱㅏ', 515: 'ㅎㅏㄴㅈㅓㅇ', 516: 'ㅇㅕㄱㅝㄴ', 517: 'ㅁㅣ', 518: 'ㅈㅏㅇㅇㅐㅇㅣㄴ', 519: 'ㄷㅡㅇㄹㅗㄱㅈㅡㅇ', 520: 'ㅋㅏㄷㅡ', 521: 'ㄱㅓㅁㅊㅏㄹ', 522: 'ㅊㅜㄹㅇㅣㅂㄱㅜㄱ', 523: 'ㄱㅣㅅㅜㄹ', 524: 'ㄴㅗㅇㅇㅓㅂ', 525: 'ㅅㅜㅅㅏㄴ', 526: 'ㄱㅣㅅㅏㅇ', 527: 'ㅂㅏㅇㅈㅐ', 528: 'ㅇㅏㄴㅈㅓㄴ', 529: 'ㄷㅏㄴㅣ', 530: 'ㅈㅣㄱㅈㅏㅇ', 531: 'ㄱㅏㅈ', 532: 'ㄱㅕㅁㅈㅣㄱ', 533: 'ㄱㅡㅁㅈㅣ', 534: 'ㅁㅗㄱㅈㅓㄱ', 535: 'ㄹㅏㄱㅗ', 536: 'ㅇㅡㅁㅕㄴ', 537: 'ㅂㅕㄴㅎㅕㄱ', 538: 'ㄱㅣㅇㅓㅂ', 539: 'ㅈㅜㅇㅅㅗㄱㅣㅇㅓㅂ', 540: 'ㄷㅐㄱㅣㅇㅓㅂ', 541: 'ㅈㅣㅂㅏㅇ', 542: 'ㅈㅏㅊㅣ', 543: 'ㄷㅏㄴㅊㅔ', 544: 'ㅊㅗ', 545: 'ㄱㅐㅇㅣㄴ', 546: 'ㅈㅗㅇㅎㅏㅂ', 547: 'ㅎㅐ', 548: 'ㄱㅗㅇㄱㅜㄴ', 549: 'ㄱㅗㅇㄱㅗㅇ', 550: 'ㅎㅜㄴㄹㅕㄴ', 551: 'ㅅㅓㅇㅈㅏㅇ', 552: 'ㅂㅏㄹㅈㅓㄴ', 553: 'ㅇㅣㅂㅅㅏ', 554: 'ㅈㅣㅊㅣㅁ', 555: 'ㅂㅏㄱㅅㅏ', 556: 'ㅎㅘㄹㄱㅣ', 557: 'ㅎㅕㄹ', 558: 'ㅅㅣㄹㅊㅓㄴ', 559: 'ㅅㅣㄱㅣ', 560: 'ㄱㅝㄴㅇㅠ', 561: 'ㅅㅓㄱㅘㄴ', 562: 'ㅇㅣㅀ', 563: 'ㅇㅜㄴㄷㅗㅇ', 564: 'ㄴㅐㄱㅗㅇ', 565: 'ㅇㅏㄴㄴㅕㅇ', 566: 'ㅇㅕㅎㅏㄱㅅㅐㅇ', 567: 'ㅇㅕㅉㅜ', 568: 'ㄹㄱㅔ', 569: 'ㄱㅣㅂㅗㄴㅈㅓㄱ', 570: 'ㅈㅓㄴㄱㅛ', 571: 'ㅅㅓㄱㅊㅏ', 572: 'ㅈㅓㄴㅁㅜㄴㅈㅓㄱ', 573: 'ㄸㅏㄹㅗ', 574: 'ㅅㅏㅇㄱㅘㄴ', 575: 'ㄱㅘㅎㅏㄱㄱㅗ', 576: 'ㅇㅓㄸㅓ', 577: 'ㄱㅏㄴㄷㅏㄴ', 578: 'ㅅㅓㄹㅁㅕㅇ', 579: 'ㄱㅏㅇㅑ', 580: 'ㅇㅣㅁㅎㅏ', 581: 'ㄱㅘㄴㅅㅣㅁ', 582: 'ㄷㅏㅂ', 583: 'ㅂㅕㄴ', 584: 'ㄷㅏㄱㅏ', 585: 'ㅌㅡㄱㅅㅓㅇ', 586: 'ㅅㅓㅇㄱㅕㄱ', 587: 'ㅂㅓㅁㅎㅐㅇ', 588: 'ㄷㅗㅇㄱㅣ', 589: 'ㅇㅢㄷㅗ', 590: 'ㅂㅏㄺㅎㅣ', 591: 'ㅇㅣㅂ', 592: 'ㄷㅡㅅㅇㅣ', 593: 'ㄱㅣㅂㅗㄴ', 594: 'ㄹㅏㄴ', 595: 'ㅂㅓㅂㅇㅢㅎㅏㄱ', 596: 'ㅅㅐㅇㅁㅜㄹㅎㅏㄱ', 597: 'ㅎㅘㅎㅏㄱ', 598: 'ㅁㅜㄹㄹㅣㅎㅏㄱ', 599: 'ㄷㅗㄱㅁㅜㄹㅎㅏㄱ', 600: 'ㅎㅕㄹㅊㅓㅇㅎㅏㄱ', 601: 'ㅈㅏㅇㅕㄴ', 602: 'ㅅㅣㅁㄹㅣㅎㅏㄱ', 603: 'ㅅㅏㅎㅚㅎㅏㄱ', 604: 'ㅊㅓㄹㅎㅏㄱ', 605: 'ㄴㅗㄴㄹㅣㅎㅏㄱ', 606: 'ㅅㅏㅎㅚㄱㅘㅎㅏㄱ', 607: 'ㅈㅣㅅㅣㄱ', 608: 'ㄱㅣㄱㅜ', 609: 'ㅇㅣㅇㅛㅇ', 610: 'ㅊㅔㄱㅖㅈㅓㄱ', 611: 'ㅎㅏㅂㄹㅣㅈㅓㄱ', 612: 'ㅇㅕㄴㄱㅜㅇㅝㄴ', 613: 'ㅅㅜㅅㅏㄷㅐ', 614: 'ㅅㅜㅅㅏㅂㅜ', 615: 'ㅇㅕㄴㄱㅜㅅㅗ', 616: 'ㅇㅕㄴㄱㅜㅅㅏ', 617: 'ㅇㅕㄴㄱㅜㄱㅘㄴ', 618: 'ㅎㅗㅊㅣㅇ', 619: 'ㄱㅗㅇㅌㅗㅇㅈㅓㅁ', 620: 'ㅇㅢㅅㅏ', 621: 'ㅂㅓㅂㅇㅢ', 622: 'ㅅㅐㅇㅁㅜㄹㅎㅏㄱㅈㅏ', 623: 'ㅎㅘ', 624: 'ㅁㅜㄹㄹㅣㅎㅏㄱㅈㅏ', 625: 'ㄷㅗㄱㅁㅜㄹ', 626: 'ㅎㅕㄹㅊㅓㅇ', 627: 'ㅅㅣㅁㄹㅣㅎㅏㄱㅈㅏ', 628: 'ㅅㅏㅎㅚㅎㅏㄱㅈㅏ', 629: 'ㅊㅓㄹㅎㅏㄱㅈㅏ', 630: 'ㄴㅗㄴㄹㅣ', 631: 'ㄱㅜㅅㅓㅇ', 632: 'ㄷㅏㄴㅡㄴ', 633: 'ㅈㅏㄹㅛㅊㅓㄹ', 634: '3,104', 635: '2,820', 636: '1,961', 637: '739', 638: '284', 639: 'ㄱㅣㅊㅔ', 640: 'ㅈㅓㅇㅂㅣ', 641: 'ㅍㅣㅎㅐㅈㅏ', 642: 'ㅅㅣㅁㄹㅣ', 643: 'ㅌㅡㄱㄱㅗㅇㄷㅐ', 644: 'ㅊㅓㄹㅣ', 645: 'ㅈㅓㄴㅅㅜ', 646: '41', 647: 'ㄱㅛㅎㅑㅇㅇㅏㄱㄷㅏㄴ', 648: 'ㅍㅣㅇㅏㄴㅗ', 649: 'ㅂㅏㅇㅣㅇㅗㄹㄹㅣㄴ', 650: 'ㅂㅣㅇㅗㄹㄹㅏ', 651: 'ㅅㅐㄱㅅㅗㅍㅗㄴ', 652: 'ㅌㅡㄹㅓㅁㅂㅗㄴ', 653: 'ㅂㅔㅇㅣㅅㅡ', 654: 'ㅈㅜㅇㄱㅜㄱㅇㅓ', 655: 'ㄴㅣㅇㅓ', 656: 'ㅋㅏㅈㅏㅎㅡ', 657: 'ㅅㅣㅇ', 658: 'ㄹㅣㅇㅓ', 659: 'ㅇㅣㄹㅂㅗㄴ', 660: 'ㅈㅜㅇㄱㅜㄱ', 661: 'ㅇㅕㅇㅅㅏㅇ', 662: '87', 663: '2,785', 664: '2,248', 665: '1,546', 666: '582', 667: 'ㅂㅕㄴㅎㅗㅅㅏ', 668: '537', 669: 'ㄱㅗㅇㅇㅣㄴ', 670: 'ㅎㅚㄱㅖㅅㅏ', 671: 'ㅊㅓㅇㅈㅏㅇ', 672: 'ㄱㅣㅁㅜ', 673: 'ㅅㅏㄱㅕㄱ', 674: '32', 675: 'ㅌㅐㄱㅝㄴ', 676: 'ㅂㅗㄱㅅㅣㅇ', 677: 'ㄹㅔㅅㅡㄹㄹㅣㅇ', 678: 'ㅈㅐㄴㅏㄴ', 679: 'ㅇㅢㄹㅛ', 680: 'ㅎㅘㅈㅐ', 681: 'ㅂㅗㅇㅏㄴ', 682: 'ㅁㅏㅇㅑㄱ', 683: '130', 684: 'ㄱㅛㅌㅗㅇ', 685: 'ㄱㅗㅇㅎㅏㄱ', 686: 'ㅂㅓㅂㅎㅏㄱ', 687: '180', 688: 'ㅊㅏㄹㅖ', 689: 'ㅈㅏㄱㄴㅕㄴ', 690: 'ㅇㅠㅅㅏ', 691: '5,889', 692: 'ㅂㅏㅇ', 693: 'ㅇㅕㄱ', 694: 'ㄷㅗㅇ', 695: 'ㄹㅣㅁㅗㄷㅔㄹㄹㅣㅇ', 696: 'ㄱㅗㅇㅅㅏ', 697: 'ㅅㅣㅈㅓㅁ', 698: 'ㅊㅚㄷㅐ', 699: 'ㅅㅜㅇㅛㅇ', 700: 'ㅇㅑㄱ', 701: '2,400', 702: 'ㅇㅝㄴㅎㅘㄹ', 703: 'ㅇㅣㄹㅂㅜ', 704: 'ㄷㅐㄱㅣ', 705: 'ㄷㅐㅅㅏㅇㅈㅏ', 706: 'ㅅㅜㄷㅗ', 707: 'ㅇㅣㅆㅇㅡㅁ', 708: 'ㅂㅜㅇㅢ', 709: 'ㅇㅢㅁㅜㅈㅓㄱ', 710: 'ㅅㅏㅈㅣ', 711: 'ㅇㅘㄴㅈㅓㄴ', 712: 'ㄷㅏㅁ', 713: 'ㅁㅏㅁㅜㄹㅣ', 714: 'ㄷㅏㄴㄱㅖ', 715: '2.5', 716: 'ㄱㅗㅇ', 717: 'ㅎㅏㄴㅍㅕㄴ', 718: 'ㅅㅔㅂㅜㅈㅓㄱ', 719: 'ㄷㅡㅇㅇㅔ', 720: 'ㅅㅐ', 721: 'ㄹㅗㅂ', 722: 'ㅇㅗㄹㅎㅐ', 723: 'ㅅㅏㄱㅈㅔ', 724: 'ㅁㅣㅍㅣㄹㅈㅏ', 725: 'ㄱㅐㅍㅕㄴ', 726: 'ㄱㅜㅂㅜㄴ', 727: 'ㅎㅕㄴ', 728: 'ㅎㅐㅇ', 729: 'ㄱㅕㅇㅎㅐㅇ', 730: 'ㅂㅜㄹ', 731: '470', 732: '52', 733: '241', 734: '457', 735: '510', 736: 'ㅇㅏㅅㅓ', 737: 'ㅇㅕㅇㅑ', 738: 'ㅎㅚㅇㅝㄴ', 739: 'ㄱㅏㅇㅣㅂ', 740: 'ㅁㅏㅇㅣ', 741: 'ㅍㅔㅇㅣㅈㅣ', 742: 'ㅈㅏㅊㅔ', 743: 'ㅂㅜㄹㄱㅏㄴㅡㅇ', 744: 'ㅎㅏㅇㅕ', 745: 'ㅇㅏㄴㄷㅚㅁ', 746: 'ㅅㅓㅇㅈㅓㄱㅍㅛ', 747: 'ㅂㅏㄹㅅㅗㅇ', 748: '02', 749: '3150', 750: '2732', 751: 'ㅇㅕㄴㄹㅏㄱ', 752: 'ㅂㅜㄹㅍㅣㄹㅇㅛ', 753: 'ㅁㅕㄴㅈㅔ', 754: 'ㄱㅗㅇㅌㅗㅇ', 755: 'ㅁㅏㄱㅏㅁㅇㅣㄹ', 756: 'ㅇㅜㄴㅈㅓㄴㅁㅕㄴㅎㅓ', 757: 'ㅂㅗㅌㅗㅇ', 758: 'ㄷㅐㅎㅕㅇ', 759: 'ㅁㅕㄴㅎㅓ', 760: 'ㅅㅗㅈㅣㅈㅏ', 761: 'ㅇㅣㅅ', 762: '39', 763: '27', 764: '45', 765: 'ㅎㅏㅁㅈㅓㅇ', 766: 'ㅇㅢㅁㅜ', 767: 'ㅇㅜ', 768: 'ㅁㅏㅈㅣㅁㅏㄱ', 769: 'ㅇㅣㅂㄷㅐ', 770: 'ㅈㅓㄴㅇㅕㄱ', 771: 'ㅂㅜㅈㅗㄱㅂㅜㄴ', 772: 'ㅊㅜㅇㅇㅝㄴ', 773: 'ㅈㅜㄹㅇㅓㄷㅡㄹ', 774: 'ㅌㅡㄱㅎㅖ', 775: 'ㄹㅕ', 776: 'ㅇㅛㄱㅜ', 777: 'ㅅㅐㄹㅗㅂ', 778: 'ㅇㅝㄴㄹㅣ', 779: 'ㄱㅣㅂㅗㄴㄱㅝㄴ', 780: 'ㅇㅣㄴㄱㅝㄴ', 781: 'ㄱㅏㅊㅣ', 782: 'ㅈㅓㅇㅅㅣㄴ', 783: 'ㅎㅏㅁㅇㅑㅇ', 784: 'ㅈㅡㅇㄱㅓ', 785: 'ㅎㅐㅇㅈㅓㅇㅎㅏㄱ', 786: 'ㅇㅣㄹㄱㅘㄹ', 787: 'ㄷㅏㅇ', 788: 'ㅇㅣㅂㅅㅣㄹ', 789: 'ㅅㅣㅈㅏㄱㅎㅏ', 790: 'ㅂㅜㅈㅗㄱ', 791: 'ㅇㅣㅇㅔ', 792: 'ㅅㅡㅁㅏㅌㅡ', 793: 'ㅍㅗㄴ', 794: 'ㅅㅡㅌㅗㅂ', 795: 'ㅊㅣㄹㅡ', 796: 'ㄱㅏㄷㅗㅇ', 797: 'ㅇㅕㄴㅅㅡㅂ', 798: 'ㅁㅏㄶㅇㅣ', 799: 'ㅂㅗㅅㅔ', 800: 'ㅍㅛ', 801: 'ㅅㅏㅇㄱㅣ', 802: 'ㅇㅏㄹㄹㅣ', 803: 'ㅅㅏㅅㅏㅇ', 804: 'ㄱㅓㄴㅈㅓㄴ', 805: 'ㅍㅜㅁㅎㅐㅇ', 806: 'ㅂㅏㅇㅈㅓㅇ', 807: 'ㄷㅐㅎㅏㄴㅁㅣㄴㄱㅜㄱ', 808: 'ㄱㅜㄱㅈㅓㄱㅂㅓㅂ', 809: 'ㅍㅣㅎㅏ', 810: 'ㄷㅡㅇㅈㅐ', 811: 'ㄹㅏㄷㅗ', 812: 'ㅅㅗㅁㅕㄹ', 813: 'ㅂㅕㅇㅇㅕㄱㅂㅓㅂ', 814: 'ㅈㅣㅇㅈㅣㅂ', 815: 'ㅅㅗㅈㅣㅂ', 816: 'ㅎㅏㄱㅇㅓㅂ', 817: 'ㄱㅖㅅㅗㄱ', 818: 'ㄱㅐㅇㅝㄹ', 819: 'ㅈㅏㅇㄱㅣ', 820: 'ㅇㅛㅇㅑㅇ', 821: 'ㅇㅣㅁㅅㅣㄴ', 822: 'ㅊㅜㄹㅅㅏㄴ', 823: 'ㅂㅏㄲ', 824: 'ㅂㅜㄷㅡㄱㅇㅣ', 825: 'ㅈㅡㅇㅁㅕㅇ', 826: 'ㅊㅓㅁㅂㅜ', 827: 'ㅂㅜㄴㅁㅕㅇ', 828: 'ㅎㅜㅂㅗ', 829: 'ㅇㅗㄹㄹㅣ', 830: 'ㄷㅡㄹㅣ', 831: 'ㅊㅣㄹㅜ', 832: 'ㅎㅏㄱㄱㅜㄴㄷㅏㄴ', 833: 'ㅊㅓㄹㅓㅁ', 834: 'ㄱㅣㅇㅝㄴ', 835: 'ㄷㅐㅎㅏㄱㅅㅐㅇ', 836: 'ㄱㅣㅎㅚㄱ', 837: 'ㅎㅕㅂㄹㅕㄱ', 838: 'ㅅㅏㅇㄷㅏㅁ', 839: 'ㅅㅓㅇㅂㅕㄹ', 840: 'ㅂㅜㄴㄹㅣ', 841: '88', 842: '90', 843: '80', 844: 'ㄴㅗㅇㅇㅓㅊㅗㄴ', 845: 'ㅁㅜㄱㅜㅇㅎㅘ', 846: 'ㅇㅟㅇㅝㄴ', 847: 'ㅎㅚㅇㅢ', 848: 'ㅅㅣㅁㅇㅢ', 849: 'ㅇㅢㄱㅕㄹ', 850: 'ㄱㅓㅊㅣ', 851: 'ㅂㅜㄹㄱㅏ', 852: '42', 853: 'ㄴㅓㅁ', 854: 'ㅊㅜㄹㅅㅐㅇ', 855: '16', 856: 'ㅅㅣㅎㅐㅇㄹㅕㅇ', 857: 'ㅈㅜㄴㅇㅛㅇ', 858: 'ㅇㅕㄴㅈㅏㅇ', 859: 'ㄱㅐㄱㄱㅘㄴㅅㅣㄱ', 860: 'ㅌㅐㄱㅇㅣㄹ', 861: 'ㄷㅏㄴㄷㅏㅂㅎㅕㅇ', 862: 'ㅈㅜㄱㅘㄴㅅㅣㄱ', 863: 'ㅁㅜㄴㅎㅏㅇ', 864: 'ㅂㅐㅈㅓㅁ', 865: 'ㅈㅓㅇㅇㅝㄴ', 866: '400', 867: 'ㄱㅕㄹㅇㅝㄴ', 868: 'ㅂㅏㄹㅅㅐㅇ', 869: 'ㅎㅏㄴㅎㅏ', 870: 'ㅎㅏㅁ', 871: 'ㅁㅣㄷㅏㄹ', 872: 'ㄷㅏㄴㄱㅖㅂㅕㄹ', 873: 'ㄱㅕㄹㅈㅓㅇ', 874: 'ㅎㅘㄹㅇㅛㅇ', 875: 'ㅎㅏㄱㄱㅛㅅㅐㅇㅎㅘㄹ', 876: '150', 877: '500', 878: 'ㅎㅏㄱㅅㅐㅇㅂㅜ', 879: 'ㅎㅏㄱㄱㅣ', 880: 'ㅇㅕㅇㅇㅕㄱ', 881: 'ㅈㅓㅇㅅㅣ', 882: 'ㅈㅓㄱㅇㅛㅇ', 883: 'ㅎㅘㄱ', 884: 'ㅇㅘㅇㅂㅗㄱ', 885: 'ㅇㅗㄹㅐ', 886: 'ㄷㅏㄹㄹㅣㄱㅣ', 887: 'ㅂㅗㄷㅏ', 888: 'ㅅㅏㅇㅎㅑㅇ', 889: 'ㅈㅗㅈㅓㅇ', 890: 'ㅊㅏㅁㅈㅗ', 891: 'ㅇㅕㅅㅓㅇ', 892: 'ㄷㅗㅇㅇㅣㄹ', 893: 'ㅁㅜㄹㅡㅍ', 894: 'ㄸㅔ', 895: 'ㅍㅕㄴㅇㅣㅂㅅㅐㅇ', 896: 'ㄷㅗㅇㅅㅣ', 897: 'ㅇㅣㄴㅈㅡㅇ', 898: 'ㄴㄷㅏㄴㅡㄴ', 899: 'ㅁㅏㄹㅆㅡㅁ', 900: 'ㅈㅛ', 901: 'ㄱㅕㅇㅁㅜㄱㅘㄴ', 902: 'ㅊㅣㅇㅏㄴㄱㅏㅁ', 903: 'ㅈㅓㅇㄱㅏㅁ', 904: 'ㅌㅓ', 905: 'ㅊㅟㄱㅡㅂ', 906: 'ㅎㅐㅂㅏㅇ', 907: 'ㅅㅣㄱㅁㅣㄴㅈㅣ', 908: 'ㅈㅏㄴㅈㅐ', 909: 'ㅂㅜㄹㅅㅣㄱ', 910: 'ㅁㅣㄴㅈㅜㅎㅘ', 911: 'ㅅㅣㄹㅎㅕㄴ', 912: 'ㄱㅗㅈㅏ', 913: '1947', 914: 'ㅊㅚㅊㅗ', 915: 'ㄱㅗㄱㅡㅂ', 916: 'ㅌㅏㄴㅅㅐㅇ', 917: '93', 918: 'ㅅㅣㅈㅏㄱ', 919: 'ㅂㅐㅊㅜㄹ', 920: 'ㅈㅣㄹㅈㅓㄱ', 921: 'ㅎㅑㅇㅅㅏㅇ', 922: 'ㅋㅡㄴㅏㅋㅡ', 923: 'ㄱㅣㅇㅕ', 924: 'ㅎㅏㄴㄱㅜㄱ', 925: 'ㅁㅐㄱ', 926: 'ㅈㅜㅇㄱㅏㄴ', 927: 'ㅇㅣㅂㅈㅣㄱ', 928: 'ㅂㅏㅇㅊㅣㅁ', 929: '2019', 930: 'ㅌㅏㄱㅇㅝㄹ', 931: 'ㅇㅜㅅㅜ', 932: 'ㅇㅕㄴㅕㄴ', 933: 'ㄱㅕㅇㅎㅓㅁ', 934: 'ㅅㅏㄹㄹㅣ', 935: 'ㅂㅕㄴㅎㅘ', 936: 'ㅅㅓㅇㄱㅘ', 937: 'ㄱㅜㄱㅁㅣㄴ', 938: 'ㅅㅗㅌㅗㅇ', 939: 'ㄱㅗㅇㄱㅏㅁ', 940: 'ㄱㅏㅁㅅㅜㅅㅓㅇ', 941: 'ㄱㅕㅁㅂㅣ', 942: 'ㅂㅏㄹㅡ', 943: 'ㅇㅣㄴㅅㅓㅇ', 944: 'ㄱㅏㅈㅊㅜ', 945: 'ㄹㅗㅅㅓ', 946: 'ㄱㅣㅎㅚ', 947: 'ㄱㅗㅇㅍㅕㅇ', 948: 'ㅈㅜㅇㅓㅈㅣ', 949: 'ㄱㅣㄹ', 950: 'ㄱㅓㄷ', 951: 'ㅈㅓㄴㅎㅏ', 952: 'ㅂㅅㅣㅇㅗ', 953: '!', 954: 'ㅇㅕㄹㅈㅓㅇ', 955: 'ㄲㅜㅁ', 956: 'ㅇㅡㅇㅇㅝㄴ', 957: 'ㅌㅡㄱㅈㅓㄴ', 958: 'ㄱㅓㄷㅡㅂㄴㅏ', 959: 'ㅎㅏㄱㅅㅡㅂ', 960: 'ㅅㅗ', 961: 'ㅈㅓㅇㅇㅢ', 962: 'ㅅㅜㄷㅏㅇ', 963: 'ㅁㅐㅇㅝㄹ', 964: 'ㅍㅣㅂㅗㄱ', 965: 'ㅊㅣㅁㄱㅜ', 966: 'ㄱㅛㄱㅘㅅㅓ', 967: 'ㄱㅡㅂㅇㅕ', 968: 'ㅍㅜㅁ', 969: 'ㅅㅜㅅㅏㅇ', 970: 'ㅇㅣㄴㅁㅕㅇ', 971: 'ㄱㅜㅈㅗ', 972: 'ㅈㅏㄱㅕㄱㅈㅡㅇ', 973: 'ㅁㅜㄷㅗ', 974: 'ㄷㅏㅎ', 975: 'ㅌㅐㄱㅝㄴㄷㅗ', 976: 'ㅎㅏㅂㄱㅣㄷㅗ', 977: 'ㅋㅓㅁㅍㅠㅌㅓ', 978: 'ㅅㅜㄱㄷㅏㄹ', 979: 'ㄷㅏㅇㅑㅇ', 980: 'ㅂㅐㅇㅜ', 981: 'ㅈㅣㄴㄹㅗ', 982: 'ㅅㅜㄹㅛ', 983: 'ㅈㅣㄱㅜㄷㅐ', 984: 'ㅍㅏㅊㅜㄹㅅㅗ', 985: 'ㄱㅕㅇㅊㅏㄹㅅㅓ', 986: 'ㄱㅕㅇㅈㅔ', 987: 'ㅌㅣㅁ', 988: 'ㄱㅗㄹㅕ', 989: 'ㅂㅐㅊㅣ', 990: 'ㅌㅡㄱㅅㅜ', 991: 'ㅇㅣㄹㅜㅇㅓㅈㅣ', 992: 'ㅂㅕㅇㅎㅐㅇ', 993: 'ㅊㅚㅈㅓ', 994: 'ㅈㅜㅅㅗ', 995: 'ㄱㅗㄴㅡㄴ', 996: 'ㅎㅢㅁㅏㅇㅈㅏ', 997: ',,,,', 998: 'ㅈㅣㄹㅡ', 999: 'ㅎㅜㅇㅢ', 1000: 'ㅅㅜㄴㅎㅘㄴ', 1001: 'ㅇㅕㅅㅓ', 1002: 'ㄱㅕㄹ', 1003: '33', 1004: 'ㅅㅣㄹㅎㅕㅇ', 1005: 'ㅈㅗㅇㄹㅛ', 1006: 'ㄲㅡㅌㄴㅏ', 1007: 'ㅂㅓㅂㅇㅝㄴ', 1008: 'ㅍㅏㄴㄱㅕㄹ', 1009: 'ㅅㅏㅇㅅㅣㄹ', 1010: 'ㅂㅏㅇㅅㅣㄱ', 1011: 'ㅇㅣㄹㅈㅏ', 1012: 'ㅈㅜㅇㅢ', 1013: 'ㅈㅜㅁㅣㄴㄷㅡㅇㄹㅗㄱㅈㅡㅇ', 1014: 'ㅇㅜㄴㅈㅓㄴ', 1015: 'ㅁㅕㄴㅎㅓㅈㅡㅇ', 1016: 'ㅈㅡㅇㅁㅕㅇㅅㅓ', 1017: 'ㅊㅜ', 1018: 'ㅎㅏㄱㅅㅐㅇㅈㅡㅇ', 1019: 'ㅅㅜㅊㅓㅂ', 1020: 'ㅈㅣㅊㅏㅁ', 1021: 'ㅇㅣㄴㅎㅏ', 1022: 'ㅂㅜㄹㅇㅣㅇㅣㄱ', 1023: 'ㅂㅗㄴㅇㅣㄴ', 1024: 'ㅊㅐㄱㅇㅣㅁ', 1025: 'ㅂㅏㄲㅟ', 1026: 'ㅁㅏㅈㅊㅜ', 1027: 'ㅊㅓㅇㅡㅁ', 1028: 'ㄱㅗㅇㅂㅜ', 1029: 'ㅁㅏㅈ', 1030: 'ㅈㅣㅇㅝㄴㅅㅓ', 1031: 'ㅂㅓㅂㅁㅜ', 1032: 'ㄱㅜㄱㅈㅔ', 1033: 'ㅌㅗㅇㅅㅏㅇ', 1034: 'ㄱㅛㅈㅓㅇㅈㅣㄱ', 1035: 'ㄱㅛㅈㅓㅇ', 1036: 'ㄱㅘㄴㄹㅣㅈㅣㄱ', 1037: 'ㄱㅣㄱㅖ', 1038: 'ㅎㅘㄱㅗㅇ', 1039: 'ㅇㅣㅁㅇㅓㅂ', 1040: 'ㅅㅏㄴㄹㅣㅁ', 1041: 'ㅎㅐㅇㅑㅇ', 1042: 'ㅈㅗㄱㅕㅇ', 1043: 'ㅂㅏㅇㅅㅗㄴ', 1044: 'ㅇㅚㄱㅛㄱㅘㄴ', 1045: 'ㅇㅚㄱㅛ', 1046: 'ㄱㅜㄴㅇㅣㅂ', 1047: 'ㄷㅐㅈㅓㄴ', 1048: 'ㄷㅐㄱㅏ', 1049: 'ㄱㅜㄴㄷㅐ', 1050: 'ㅂㅗㄱ', 1051: 'ㅈㅣㄱㄱㅖ', 1052: 'ㅂㅗㄱㅈㅣㄱ', 1053: '..', 1054: 'ㅅㅣㄱ', 1055: 'ㅇㅣㅁㅁㅕㅇㅈㅏㅇ', 1056: 'ㅈㅓㅇㅅㅣㄱ', 1057: '64', 1058: 'ㄱㅗㅇㅁㅜ', 1059: 'ㅇㅚ', 1060: 'ㅈㅗㅇㅅㅏ', 1061: 'ㅅㅗㅅㅗㄱ', 1062: 'ㄱㅣㄱㅘㄴㅈㅏㅇ', 1063: 'ㅎㅓㄱㅏ', 1064: 'ㄱㅕㅁㅎㅏ', 1065: 'ㅎㅏㄴㄱㅖ', 1066: 'ㄷㅐㅌㅗㅇㄹㅕㅇㄹㅕㅇ', 1067: 'ㄴㅡㄴㅑ', 1068: 'ㄴㅡ', 1069: 'ㅅㅓㄹㅠ', 1070: 'ㅊㅜㄹㅎㅏ', 1071: 'ㅅㅏㅍㅛ', 1072: 'ㅊㅗㅇㅁㅜㄱㅘ', 1073: 'ㅁㅏㄹㅆㅡㅁㄷㅡㄹㅣ', 1074: 'ㅍㅏㄴ', 1075: 'ㅁㅗㅁ', 1076: 'ㅎㅐㅇㅂㅗㄱ', 1077: 'ㅅㅏㄻ', 1078: 'ㅇㅣㄺ', 1079: 'ㄱㅜㄱㅎㅚ', 1080: 'ㄱㅜㄱㄱㅗㅇㄹㅣㅂ', 1081: 'ㄷㅗㅅㅓㄱㅘㄴ', 1082: 'ㄱㅏㅅㅣ', 1083: 'ㅁㅜㄹㅛ', 1084: 'ㅇㅕㄹㄹㅏㅁ', 1085: 'ㅂㅕㄴㄷㅓㄱ', 1086: 'ㅅㅡㄹㅓㅂ', 1087: 'ㄱㅖㅈㅓㄹ', 1088: '....', 1089: 'ㄱㅏㅁㄱㅣ', 1090: 'ㄷㅗㄱㄱㅏㅁ', 1091: 'ㅈㅗㅅㅣㅁ', 1092: 'ㅁㅏㅅㅡㅋㅡ', 1093: 'ㅁㅗㄷㅡㄴ', 1094: 'ㅍㅕㅇㅅㅗ', 1095: 'ㅇㅕㅇㅇㅑㅇ', 1096: 'ㅅㅜㅁㅕㄴ', 1097: 'ㅅㅣㅂㅇㅣㄷㅏㄴ', 1098: 'ㅇㅑㅇㅅㅐㅇ', 1099: 'ㄱㅣㅎㅕㄹ', 1100: 'ㅈㅣㅇㅏㅂ', 1101: 'ㅁㅏㅅㅏㅈㅣ', 1102: '14', 1103: 'ㄱㅕㅇㅁㅐㄱ', 1104: '365', 1105: 'ㄱㅕㅇㅎㅕㄹ'}\n",
            "0\n",
            "7819\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-149-be30e8b92f8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;31m#gen_sample.write('pre-training...\\n')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPRE_EPOCH_NUM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpre_train_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_embedding_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m5\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0mgenerate_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerated_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_embedding_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-149-be30e8b92f8f>\u001b[0m in \u001b[0;36mpre_train_epoch\u001b[0;34m(sess, trainable_model, data_loader, word_embedding_matrix)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0mdata_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_pointer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainable_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_embedding_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Gen_Data_loader' object has no attribute 'num_batch'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVLalmvN3Nyq"
      },
      "source": [
        "생성자,감별자 모델 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1hyOa7XNh1k"
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "import numpy as np\n",
        "from tensorflow.python.ops import tensor_array_ops, control_flow_ops\n",
        "\n",
        "class Generator(object):\n",
        "    def __init__(self, num_emb, batch_size, emb_dim, hidden_dim,\n",
        "                 sequence_length, start_token,\n",
        "                 learning_rate=0.01, reward_gamma=0.95):\n",
        "        self.num_emb = num_emb\n",
        "        self.batch_size = batch_size\n",
        "        self.emb_dim = emb_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.sequence_length = sequence_length\n",
        "        self.start_token_input = np.array([start_token] * self.batch_size, dtype=np.int32)\n",
        "        self.word_embedding_matrix = tf.placeholder(dtype=tf.float32, shape=[num_emb, emb_dim], name='word_embed')\n",
        "        self.learning_rate = tf.Variable(float(learning_rate), trainable=False)\n",
        "        self.reward_gamma = reward_gamma\n",
        "        self.g_params = []\n",
        "        self.d_params = []\n",
        "        self.temperature = 1.0\n",
        "        self.grad_clip = 5.0\n",
        "\n",
        "        self.expected_reward = tf.Variable(tf.zeros([self.sequence_length]))\n",
        "\n",
        "        with tf.variable_scope('generator'):\n",
        "            self.g_embeddings = self.word_embedding_matrix\n",
        "            self.g_recurrent_unit = self.create_recurrent_unit(self.g_params)  # maps h_tm1 to h_t for generator\n",
        "            self.g_output_unit = self.create_output_unit(self.g_params)  # maps h_t to o_t (output token logits)\n",
        "\n",
        "        # placeholder definition\n",
        "        self.start_token = tf.placeholder(tf.int32, shape=[self.batch_size])\n",
        "        self.x = tf.placeholder(tf.int32, shape=[self.batch_size, self.sequence_length]) # sequence of tokens generated by generator\n",
        "        self.rewards = tf.placeholder(tf.float32, shape=[self.batch_size, self.sequence_length]) # get from rollout policy and discriminator\n",
        "\n",
        "        # processed for batch\n",
        "        with tf.device(\"/cpu:0\"):\n",
        "            self.processed_x = tf.transpose(tf.nn.embedding_lookup(self.g_embeddings, self.x), perm=[1, 0, 2])  # seq_length x batch_size x emb_dim\n",
        "\n",
        "        # Initial states\n",
        "        self.h0 = tf.zeros([self.batch_size, self.hidden_dim])\n",
        "        self.h0 = tf.stack([self.h0, self.h0])\n",
        "\n",
        "        gen_o = tensor_array_ops.TensorArray(dtype=tf.float32, size=self.sequence_length,\n",
        "                                             dynamic_size=False, infer_shape=True)\n",
        "        gen_x = tensor_array_ops.TensorArray(dtype=tf.int32, size=self.sequence_length,\n",
        "                                             dynamic_size=False, infer_shape=True)\n",
        "\n",
        "        def _g_recurrence(i, x_t, h_tm1, gen_o, gen_x):\n",
        "            h_t = self.g_recurrent_unit(x_t, h_tm1)  # hidden_memory_tuple\n",
        "            o_t = self.g_output_unit(h_t)  # batch x vocab , logits not prob\n",
        "            log_prob = tf.log(tf.nn.softmax(o_t))\n",
        "            next_token = tf.cast(tf.reshape(tf.multinomial(log_prob, 1), [self.batch_size]), tf.int32)\n",
        "            x_tp1 = tf.nn.embedding_lookup(self.g_embeddings, next_token)  # batch x emb_dim\n",
        "            gen_o = gen_o.write(i, tf.reduce_sum(tf.multiply(tf.one_hot(next_token, self.num_emb, 1.0, 0.0),\n",
        "                                                             tf.nn.softmax(o_t)), 1))  # [batch_size] , prob\n",
        "            gen_x = gen_x.write(i, next_token)  # indices, batch_size\n",
        "            return i + 1, x_tp1, h_t, gen_o, gen_x\n",
        "\n",
        "        _, _, _, self.gen_o, self.gen_x = control_flow_ops.while_loop(\n",
        "            cond=lambda i, _1, _2, _3, _4: i < self.sequence_length,\n",
        "            body=_g_recurrence,\n",
        "            loop_vars=(tf.constant(0, dtype=tf.int32),\n",
        "                       tf.nn.embedding_lookup(self.g_embeddings, self.start_token),\n",
        "                       self.h0, gen_o, gen_x))\n",
        "\n",
        "        self.gen_x = self.gen_x.stack()  # seq_length x batch_size\n",
        "        self.gen_x = tf.transpose(self.gen_x, perm=[1, 0])  # batch_size x seq_length\n",
        "\n",
        "        # supervised pretraining for generator\n",
        "        g_predictions = tensor_array_ops.TensorArray(\n",
        "            dtype=tf.float32, size=self.sequence_length,\n",
        "            dynamic_size=False, infer_shape=True)\n",
        "\n",
        "        ta_emb_x = tensor_array_ops.TensorArray(\n",
        "            dtype=tf.float32, size=self.sequence_length)\n",
        "        ta_emb_x = ta_emb_x.unstack(self.processed_x)\n",
        "\n",
        "        def _pretrain_recurrence(i, x_t, h_tm1, g_predictions):\n",
        "            h_t = self.g_recurrent_unit(x_t, h_tm1)\n",
        "            o_t = self.g_output_unit(h_t)\n",
        "            g_predictions = g_predictions.write(i, tf.nn.softmax(o_t))  # batch x vocab_size\n",
        "            x_tp1 = ta_emb_x.read(i)\n",
        "            return i + 1, x_tp1, h_t, g_predictions\n",
        "\n",
        "        _, _, _, self.g_predictions = control_flow_ops.while_loop(\n",
        "            cond=lambda i, _1, _2, _3: i < self.sequence_length,\n",
        "            body=_pretrain_recurrence,\n",
        "            loop_vars=(tf.constant(0, dtype=tf.int32),\n",
        "                       tf.nn.embedding_lookup(self.g_embeddings, self.start_token),\n",
        "                       self.h0, g_predictions))\n",
        "\n",
        "        self.g_predictions = tf.transpose(self.g_predictions.stack(), perm=[1, 0, 2])  # batch_size x seq_length x vocab_size\n",
        "\n",
        "        # pretraining loss\n",
        "        self.pretrain_loss = -tf.reduce_sum(\n",
        "            tf.one_hot(tf.to_int32(tf.reshape(self.x, [-1])), self.num_emb, 1.0, 0.0) * tf.log(\n",
        "                tf.clip_by_value(tf.reshape(self.g_predictions, [-1, self.num_emb]), 1e-20, 1.0)\n",
        "            )\n",
        "        ) / (self.sequence_length * self.batch_size)\n",
        "\n",
        "        # training updates\n",
        "        pretrain_opt = self.g_optimizer(self.learning_rate)\n",
        "\n",
        "        self.pretrain_grad, _ = tf.clip_by_global_norm(tf.gradients(self.pretrain_loss, self.g_params), self.grad_clip)\n",
        "        self.pretrain_updates = pretrain_opt.apply_gradients(zip(self.pretrain_grad, self.g_params))\n",
        "\n",
        "        #######################################################################################################\n",
        "        #  Unsupervised Training\n",
        "        #######################################################################################################\n",
        "        self.g_loss = -tf.reduce_sum(\n",
        "            tf.reduce_sum(\n",
        "                tf.one_hot(tf.to_int32(tf.reshape(self.x, [-1])), self.num_emb, 1.0, 0.0) * tf.log(\n",
        "                    tf.clip_by_value(tf.reshape(self.g_predictions, [-1, self.num_emb]), 1e-20, 1.0)\n",
        "                ), 1) * tf.reshape(self.rewards, [-1])\n",
        "        )\n",
        "\n",
        "        g_opt = self.g_optimizer(self.learning_rate)\n",
        "\n",
        "        self.g_grad, _ = tf.clip_by_global_norm(tf.gradients(self.g_loss, self.g_params), self.grad_clip)\n",
        "        self.g_updates = g_opt.apply_gradients(zip(self.g_grad, self.g_params))\n",
        "\n",
        "    def generate(self, sess, word_embedding_matrix):\n",
        "        outputs = sess.run(self.gen_x, feed_dict={self.word_embedding_matrix: word_embedding_matrix,\n",
        "                                                  self.start_token: self.start_token_input})\n",
        "        return outputs\n",
        "\n",
        "    def pretrain_step(self, sess, x, word_embedding_matrix):\n",
        "        outputs = sess.run([self.pretrain_updates, self.pretrain_loss],\n",
        "                           feed_dict={self.x: x, self.word_embedding_matrix: word_embedding_matrix,\n",
        "                                      self.start_token: self.start_token_input})\n",
        "        return outputs\n",
        "\n",
        "    def init_matrix(self, shape):\n",
        "        return tf.random_normal(shape, stddev=0.1)\n",
        "\n",
        "    def init_vector(self, shape):\n",
        "        return tf.zeros(shape)\n",
        "\n",
        "    def create_recurrent_unit(self, params):\n",
        "        # Weights and Bias for input and hidden tensor\n",
        "        self.Wi = tf.Variable(self.init_matrix([self.emb_dim, self.hidden_dim]))\n",
        "        self.Ui = tf.Variable(self.init_matrix([self.hidden_dim, self.hidden_dim]))\n",
        "        self.bi = tf.Variable(self.init_matrix([self.hidden_dim]))\n",
        "\n",
        "        self.Wf = tf.Variable(self.init_matrix([self.emb_dim, self.hidden_dim]))\n",
        "        self.Uf = tf.Variable(self.init_matrix([self.hidden_dim, self.hidden_dim]))\n",
        "        self.bf = tf.Variable(self.init_matrix([self.hidden_dim]))\n",
        "\n",
        "        self.Wog = tf.Variable(self.init_matrix([self.emb_dim, self.hidden_dim]))\n",
        "        self.Uog = tf.Variable(self.init_matrix([self.hidden_dim, self.hidden_dim]))\n",
        "        self.bog = tf.Variable(self.init_matrix([self.hidden_dim]))\n",
        "\n",
        "        self.Wc = tf.Variable(self.init_matrix([self.emb_dim, self.hidden_dim]))\n",
        "        self.Uc = tf.Variable(self.init_matrix([self.hidden_dim, self.hidden_dim]))\n",
        "        self.bc = tf.Variable(self.init_matrix([self.hidden_dim]))\n",
        "        params.extend([\n",
        "            self.Wi, self.Ui, self.bi,\n",
        "            self.Wf, self.Uf, self.bf,\n",
        "            self.Wog, self.Uog, self.bog,\n",
        "            self.Wc, self.Uc, self.bc])\n",
        "\n",
        "        def unit(x, hidden_memory_tm1):\n",
        "            previous_hidden_state, c_prev = tf.unstack(hidden_memory_tm1)\n",
        "\n",
        "            # Input Gate\n",
        "            i = tf.sigmoid(\n",
        "                tf.matmul(x, self.Wi) +\n",
        "                tf.matmul(previous_hidden_state, self.Ui) + self.bi\n",
        "            )\n",
        "\n",
        "            # Forget Gate\n",
        "            f = tf.sigmoid(\n",
        "                tf.matmul(x, self.Wf) +\n",
        "                tf.matmul(previous_hidden_state, self.Uf) + self.bf\n",
        "            )\n",
        "\n",
        "            # Output Gate\n",
        "            o = tf.sigmoid(\n",
        "                tf.matmul(x, self.Wog) +\n",
        "                tf.matmul(previous_hidden_state, self.Uog) + self.bog\n",
        "            )\n",
        "\n",
        "            # New Memory Cell\n",
        "            c_ = tf.nn.tanh(\n",
        "                tf.matmul(x, self.Wc) +\n",
        "                tf.matmul(previous_hidden_state, self.Uc) + self.bc\n",
        "            )\n",
        "\n",
        "            # Final Memory cell\n",
        "            c = f * c_prev + i * c_\n",
        "\n",
        "            # Current Hidden state\n",
        "            current_hidden_state = o * tf.nn.tanh(c)\n",
        "\n",
        "            return tf.stack([current_hidden_state, c])\n",
        "\n",
        "        return unit\n",
        "\n",
        "    def create_output_unit(self, params):\n",
        "        self.Wo = tf.Variable(self.init_matrix([self.hidden_dim, self.num_emb]))\n",
        "        self.bo = tf.Variable(self.init_matrix([self.num_emb]))\n",
        "        params.extend([self.Wo, self.bo])\n",
        "\n",
        "        def unit(hidden_memory_tuple):\n",
        "            hidden_state, c_prev = tf.unstack(hidden_memory_tuple)\n",
        "            # hidden_state : batch x hidden_dim\n",
        "            logits = tf.matmul(hidden_state, self.Wo) + self.bo\n",
        "            # output = tf.nn.softmax(logits)\n",
        "            return logits\n",
        "\n",
        "        return unit\n",
        "\n",
        "    def g_optimizer(self, *args, **kwargs):\n",
        "        return tf.train.AdamOptimizer(*args, **kwargs)\n",
        "\n",
        "    def change_start_token(self, token):\n",
        "        self.start_token_input = np.array([token] * self.batch_size, dtype=np.int32)"
      ],
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ni20rpOI3JNM"
      },
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "# a = open('./data/pretrain_embedding_vec.pkl', 'rb')\n",
        "# word_embedding_matrix = pickle.load(a)\n",
        "# word_embedding_matrix = word_embedding_matrix.astype(np.float32)\n",
        "\n",
        "# An alternative to tf.nn.rnn_cell._linear function, which has been removed in Tensorfow 1.0.1\n",
        "# The highway layer is borrowed from https://github.com/mkroutikov/tf-lstm-char-cnn\n",
        "def linear(input_, output_size, scope=None):\n",
        "    '''\n",
        "    Linear map: output[k] = sum_i(Matrix[k, i] * input_[i] ) + Bias[k]\n",
        "    Args:\n",
        "    input_: a tensor or a list of 2D, batch x n, Tensors.\n",
        "    output_size: int, second dimension of W[i].\n",
        "    scope: VariableScope for the created subgraph; defaults to \"Linear\".\n",
        "  Returns:\n",
        "    A 2D Tensor with shape [batch x output_size] equal to\n",
        "    sum_i(input_[i] * W[i]), where W[i]s are newly created matrices.\n",
        "  Raises:\n",
        "    ValueError: if some of the arguments has unspecified or wrong shape.\n",
        "  '''\n",
        "\n",
        "    shape = input_.get_shape().as_list()\n",
        "    if len(shape) != 2:\n",
        "        raise ValueError(\"Linear is expecting 2D arguments: %s\" % str(shape))\n",
        "    if not shape[1]:\n",
        "        raise ValueError(\"Linear expects shape[1] of arguments: %s\" % str(shape))\n",
        "    input_size = shape[1]\n",
        "\n",
        "    # Now the computation.\n",
        "    with tf.variable_scope(scope or \"SimpleLinear\"):\n",
        "        matrix = tf.get_variable(\"Matrix\", [output_size, input_size], dtype=input_.dtype)\n",
        "        bias_term = tf.get_variable(\"Bias\", [output_size], dtype=input_.dtype)\n",
        "\n",
        "    return tf.matmul(input_, tf.transpose(matrix)) + bias_term\n",
        "\n",
        "def highway(input_, size, num_layers=1, bias=-2.0, f=tf.nn.relu, scope='Highway'):\n",
        "    \"\"\"Highway Network (cf. http://arxiv.org/abs/1505.00387).\n",
        "    t = sigmoid(Wy + b)\n",
        "    z = t * g(Wy + b) + (1 - t) * y\n",
        "    where g is nonlinearity, t is transform gate, and (1 - t) is carry gate.\n",
        "    \"\"\"\n",
        "\n",
        "    with tf.variable_scope(scope):\n",
        "        for idx in range(num_layers):\n",
        "            g = f(linear(input_, size, scope='highway_lin_%d' % idx))\n",
        "\n",
        "            t = tf.sigmoid(linear(input_, size, scope='highway_gate_%d' % idx) + bias)\n",
        "\n",
        "            output = t * g + (1. - t) * input_\n",
        "            input_ = output\n",
        "\n",
        "    return output\n",
        "\n",
        "class Discriminator(object):\n",
        "    \"\"\"\n",
        "    A CNN for text classification.\n",
        "    Uses an embedding layer, followed by a convolutional, max-pooling and softmax layer.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self, sequence_length, num_classes, word_embedding_matrix,\n",
        "            embedding_size, filter_sizes, num_filters, l2_reg_lambda=0.0):\n",
        "        # Placeholders for input, output and dropout\n",
        "        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
        "        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n",
        "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
        "\n",
        "        # Keeping track of l2 regularization loss (optional)\n",
        "        l2_loss = tf.constant(0.0)\n",
        "        \n",
        "        with tf.variable_scope('discriminator'):\n",
        "\n",
        "            # Embedding layer\n",
        "            with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
        "                self.W = tf.Variable(word_embedding_matrix, name=\"W\")\n",
        "                self.embedded_chars = tf.nn.embedding_lookup(self.W, self.input_x)\n",
        "                self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\n",
        "\n",
        "            # Create a convolution + maxpool layer for each filter size\n",
        "            pooled_outputs = []\n",
        "            for filter_size, num_filter in zip(filter_sizes, num_filters):\n",
        "                with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
        "                    # Convolution Layer\n",
        "                    filter_shape = [filter_size, embedding_size, 1, num_filter]\n",
        "                    W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
        "                    b = tf.Variable(tf.constant(0.1, shape=[num_filter]), name=\"b\")\n",
        "                    conv = tf.nn.conv2d(\n",
        "                        self.embedded_chars_expanded,\n",
        "                        W,\n",
        "                        strides=[1, 1, 1, 1],\n",
        "                        padding=\"VALID\",\n",
        "                        name=\"conv\")\n",
        "                    # Apply nonlinearity\n",
        "                    h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
        "                    # Maxpooling over the outputs\n",
        "                    pooled = tf.nn.max_pool(\n",
        "                        h,\n",
        "                        ksize=[1, sequence_length - filter_size + 1, 1, 1],\n",
        "                        strides=[1, 1, 1, 1],\n",
        "                        padding='VALID',\n",
        "                        name=\"pool\")\n",
        "                    pooled_outputs.append(pooled)\n",
        "            \n",
        "            # Combine all the pooled features\n",
        "            num_filters_total = sum(num_filters)\n",
        "            self.h_pool = tf.concat(pooled_outputs, 3)\n",
        "            self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
        "\n",
        "            # Add highway\n",
        "            with tf.name_scope(\"highway\"):\n",
        "                self.h_highway = highway(self.h_pool_flat, self.h_pool_flat.get_shape()[1], 1, 0)\n",
        "\n",
        "            # Add dropout\n",
        "            with tf.name_scope(\"dropout\"):\n",
        "                self.h_drop = tf.nn.dropout(self.h_highway, self.dropout_keep_prob)\n",
        "\n",
        "            # Final (unnormalized) scores and predictions\n",
        "            with tf.name_scope(\"output\"):\n",
        "                W = tf.Variable(tf.truncated_normal([num_filters_total, num_classes], stddev=0.1), name=\"W\")\n",
        "                b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n",
        "                l2_loss += tf.nn.l2_loss(W)\n",
        "                l2_loss += tf.nn.l2_loss(b)\n",
        "                self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"scores\")\n",
        "                self.ypred_for_auc = tf.nn.softmax(self.scores)\n",
        "                self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
        "\n",
        "            # CalculateMean cross-entropy loss\n",
        "            with tf.name_scope(\"loss\"):\n",
        "                losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)\n",
        "                self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
        "\n",
        "        self.params = [param for param in tf.trainable_variables() if 'discriminator' in param.name]\n",
        "        d_optimizer = tf.train.AdamOptimizer(1e-4)\n",
        "        grads_and_vars = d_optimizer.compute_gradients(self.loss, self.params, aggregation_method=2)\n",
        "        self.train_op = d_optimizer.apply_gradients(grads_and_vars)"
      ],
      "execution_count": 140,
      "outputs": []
    }
  ]
}